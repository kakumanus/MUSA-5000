---
title: "Homework 6: IMDB Text Mining & Sentiment Analysis"
author: "Yiming Cao, Sujan Kakumanu, Angel Sanaa Rutherford"
date: December 12 2025
number-sections: true
execute:
  cache: true
format:
  pdf:
    include-in-header:
      text: |
        \usepackage{makecell}
---
```{r setup}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
options(scipen = 999)

# Packages
if(!require(pacman)){install.packages("pacman"); library(pacman, quietly = T)}
p_load(wordcloud, tm, SnowballC, words, NbClust, stringr, dplyr, syuzhet)

```

```{r load-preprocess-data}
#| echo: false
#| message: false
#| warning: false

# myCorpus <- tm::VCorpus(VectorSource(sapply("IMDB_Dataset_short.csv", readLines)))
# 
# myCorpus <- tm_map(myCorpus, content_transformer(tolower))

my_data <- read.csv("IMDB_Dataset_short.csv", stringsAsFactors = FALSE)

my_text_source <- VectorSource(my_data$review)

myCorpus <- VCorpus(my_text_source)

myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)

myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))

myCorpus <- tm_map(myCorpus, removeWords,c("I", "br", "You,", "The", "A", "It"))

cat(content(myCorpus[[2]])[0:1], sep = "\n")
```

```{r document-term-matrix}
#| echo: false
#| message: false
#| warning: false

dtm_cleaned <- DocumentTermMatrix(myCorpus)
tm::inspect(dtm_cleaned)

m <- as.matrix(dtm_cleaned)
dim(m)
```

```{r term-distribution}
#| echo: false
#| message: false
#| warning: false

cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_cleaned$dimnames$Terms

hist(cs, breaks=100)
```

```{r wordcloud}
#| echo: false
#| message: false
#| warning: false

tab <- as.matrix(table(cs))
wordcloud(myCorpus, min.freq=500)
```
```{r remove-infrequent}
#| echo: false
#| message: false
#| warning: false

variables_to_remove <- cs < 10000

# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]

#Some books are longer, others are shorter. Let's divide the frequencies by the total number of words (after processing) in each book.
m_fin <- m_subset/rowSums(m)

#Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(m_fin)
```

```{r sentiment}
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
head(nrc, n=20L)
```

```{r review-sentiment-analysis}
#| echo: false
#| message: false
#| warning: false

review <- as.data.frame(m[1,])
review$Term <- as.vector(rownames(review))

colnames(review)[1] = "Term_Frequency"
rownames(review) <- 1:nrow(review)

nrc_sentiment <- get_nrc_sentiment(review$Term)

Review_Sentiment <- cbind(review, nrc_sentiment)

cols_to_multiply <- names(Review_Sentiment)[3:12]

# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_Sentiment[, cols_to_multiply] <- Review_Sentiment[, cols_to_multiply] * Review_Sentiment$Term_Frequency

Review_Sentiment_Total <- t(as.matrix(colSums(Review_Sentiment[,-1:-2])))
barplot(Review_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
```

# Introduction (Angel)

In this analysis, we performed text-mining techniques to movie reviews from the Internet Movie Database (IMDb) in order to quantify and visualize word trends and emotional tones across reviews. Text-mining combines data cleaning and language processing techniques, enabling researchers to systematically analyze unstructured text for meaningful patterns such as term frequency and emotional sentiments. This approach combines the ease of decreased manual effort with nuance in understanding narratives and perspectives. 

# Methods

## Data Preprocessing

We began by importing the IMDb dataset csv into R and converting the review column into a corpus object using the `tm` package. The `VectorSource` function was called in order to treat every review as a separate document. The result was a corpus which, in this context, streamlines analysis by serving as a repository of the text documents. The corpus was then preprocessed to ensure uniformity and remove noise. Specifically, all entries were transformed to lowercase, numbers and punctuation were removed, and common English stopwords were excluded. In addition, a small set of self-defined stop and non-english words ("I", "br", "You,", "The", "A", "It") were removed after additional data exploration to further reduce noise.

## Word Cloud Creation (Angel)

After preprocessing we created a document term matrix (DTM) which represents the frequency of terms across all documents. In this matrix, each row corresponds with a document, each column corresponds to a unique term, and the cell values represent the number of times the the term itself appears in a given document. 

From this DTM, two visuals were created to represent the frequency of terms: a histogram and a word cloud. In a word cloud visualization, words are displayed in font sizes proportional to the their frequency, allowing words repeated more frequently to be more visible. We used the `wordcloud` function to create our visual which takes a specified threshold for which words to display based on frequency. In analysis, we chose to only display words that appeared more than 500 times to ensure variation but also to reduce noise.

## Sentiment Analysis (Ming)

# Results

## World Cloud (Ming)

## Sentiment Analysis (Sujan)

# Discussion (Sujan)
