---
title: "Homework 6: IMDB Text Mining & Sentiment Analysis"
author: "Yiming Cao, Sujan Kakumanu, Angel Sanaa Rutherford"
date: December 12 2025
number-sections: true
execute:
  cache: true
format:
  pdf:
    include-in-header:
      text: |
        \usepackage{makecell}
---
```{r setup}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
options(scipen = 999)

# Packages
if(!require(pacman)){install.packages("pacman"); library(pacman, quietly = T)}
p_load(wordcloud, tm, SnowballC, words, NbClust, stringr, dplyr, syuzhet)

```

```{r load-preprocess-data}
#| echo: false
#| message: false
#| warning: false

# myCorpus <- tm::VCorpus(VectorSource(sapply("IMDB_Dataset_short.csv", readLines)))
# 
# myCorpus <- tm_map(myCorpus, content_transformer(tolower))

my_data <- read.csv("IMDB_Dataset_short.csv", stringsAsFactors = FALSE)

my_text_source <- VectorSource(my_data$review)

myCorpus <- VCorpus(my_text_source)

myCorpus <- tm::tm_map(myCorpus, removeNumbers)
myCorpus <- tm_map(myCorpus, removePunctuation)

myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))

myCorpus <- tm_map(myCorpus, removeWords,c("I", "br", "You,", "The", "A", "It"))

cat(content(myCorpus[[2]])[0:1], sep = "\n")
```

```{r document-term-matrix}
#| echo: false
#| message: false
#| warning: false

dtm_cleaned <- DocumentTermMatrix(myCorpus)
tm::inspect(dtm_cleaned)

m <- as.matrix(dtm_cleaned)
dim(m)
```

```{r term-distribution}
#| echo: false
#| message: false
#| warning: false

cs <- as.matrix(colSums(m))             #How many times each term appears across all documents (texts)
rownames(cs) <- dtm_cleaned$dimnames$Terms

hist(cs, breaks=100)
```

```{r wordcloud}
#| echo: false
#| message: false
#| warning: false

tab <- as.matrix(table(cs))
wordcloud(myCorpus, min.freq=500)
```
```{r remove-infrequent}
#| echo: false
#| message: false
#| warning: false

variables_to_remove <- cs < 10000

# Subset matrix frame, excluding those variables
m_subset <- m[, !variables_to_remove]

#Some books are longer, others are shorter. Let's divide the frequencies by the total number of words (after processing) in each book.
m_fin <- m_subset/rowSums(m)

#Let's scale (normalize) each of the variables (relative frequency)
m_scale <- scale(m_fin)
```

```{r sentiment}
nrc <- syuzhet::get_sentiment_dictionary(dictionary="nrc")
head(nrc, n=20L)
```

```{r review-sentiment-analysis}
#| echo: false
#| message: false
#| warning: false

review <- as.data.frame(m[1,])
review$Term <- as.vector(rownames(review))

colnames(review)[1] = "Term_Frequency"
rownames(review) <- 1:nrow(review)

nrc_sentiment <- get_nrc_sentiment(review$Term)

Review_Sentiment <- cbind(review, nrc_sentiment)

cols_to_multiply <- names(Review_Sentiment)[3:12]

# Multiply the last ten columns (sentiments) by the first column (Term_Frequency)
Review_Sentiment[, cols_to_multiply] <- Review_Sentiment[, cols_to_multiply] * Review_Sentiment$Term_Frequency

Review_Sentiment_Total <- t(as.matrix(colSums(Review_Sentiment[,-1:-2])))
barplot(Review_Sentiment_Total, las=2, ylab='Count', main='Sentiment Scores')
```

# Introduction (Angel)

# Methods

## Word Cloud Creation (Angel)

## Sentiment Analysis (Ming)

# Results

## World Cloud (Ming)

## Sentiment Analysis (Sujan)

# Discussion (Sujan)
