---
title: "Homework 2: Using Spatial Lag, Spatial Error and Geographically Weighted Regression to Predict Median House Values in Philadelphia Block Groups"
author: "Yiming Cao, Sujan Kakumanu, Angel Sanaa Rutherford"
date: October 30 2025
number-sections: true
execute:
  cache: true
format: pdf
---
```{r setup}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
options(scipen = 999)

# Packages
if(!require(pacman)){install.packages("pacman"); library(pacman, quietly = T)}
p_load(ggplot2, dplyr, sf, spdep, spgwr, tmap, spatialreg, whitestrap, lmtest, tseries, patchwork, stargazer)

```

```{r read-files}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
Regression_shpData <- st_read("../HW 1/Lecture 1 - RegressionData.shp")
#regression_data <- read.csv("../HW 1/RegressionData.csv")
```

```{r variable-creation}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
# regression_data$LNMEDHVAL<-log(regression_data$MEDHVAL)
# 
# regression_data$LNPCTBACHMOR<-log(1+regression_data$PCTBACHMOR)
# regression_data$LNNBELPOV100<-log(1+regression_data$NBELPOV100)
# regression_data$LNPCTVACANT<-log(1+regression_data$PCTVACANT)
# regression_data$LNPCTSINGLES<-log(1+regression_data$PCTSINGLES)
```

```{r global-morans-weight-matrix}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hideplots/figures

queen<-poly2nb(Regression_shpData, row.names=Regression_shpData$POLY_ID)
summary(queen)

queenlist <- nb2listw(queen, style = 'W')

moran(Regression_shpData$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`

moranMC<-moran.mc(Regression_shpData$LNMEDHVAL, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations
moranMC

moranMCres<-moranMC$res
hist(moranMCres, freq=10000000, nclass=100)   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(Regression_shpData$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')  
```

```{r local-morans}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
lmoran<-localmoran(Regression_shpData$LNMEDHVAL, queenlist)
head(lmoran) # TODO: DO WE NEED MORE THAN THIS IN REPORT?
```

```{r ols-regression}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
reg1 <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=Regression_shpData)
summary(reg1)

logLik(reg1)

stdres <- rstandard(reg1)
Regression_shpData$standardised <- stdres

ggplot(Regression_shpData) +
  geom_sf(aes(fill =  stdres), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Standardized\nResiduals")+
  labs(
    title = "Map of Standardized Regression Residuals"
  ) +
  theme_void()+
  theme(
    plot.title = element_text(face = "bold", size=16, hjust = 0.7))
```

```{r spatially-lagged-residuals}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
wt_residu <- sapply(queen, function(x) mean(stdres[x]))

plot(wt_residu, stdres)

#Note the beta coefficient of the wt_residu. 0.73235. This suggests that there is spatial autocorrelation (areas with high residuals are near similar areas)
res.lm <- lm(formula=stdres ~ wt_residu)
summary(res.lm)

#Note the small p-value of Moran's I. Same spatial autocorrelation in the scatterplot.
moran.mc(stdres, queenlist, 999, alternative="two.sided")
moran.plot(stdres, queenlist)
```

```{r spatial-lag-reg}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures

lagreg<-lagsarlm(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=Regression_shpData, queenlist)
summary(lagreg)

lagreg_res<-lagreg$residuals

lagMoranMc<-moran.mc(lagreg_res, queenlist,999, alternative="two.sided")
lagMoranMc

moran.plot(lagreg_res, queenlist)
```

```{r spatial-error-reg}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures

errreg<-errorsarlm(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=Regression_shpData, queenlist)
summary(errreg)

errreg_res <- residuals(errreg)

errMoranMc<-moran.mc(errreg_res, queenlist,999, alternative="two.sided")
errMoranMc

moran.plot(errreg_res, queenlist)
```

```{r gwr-adaptive bandwidth}
#| echo: false
#| eval: false

#Setting an adaptive bandwidth
shps <- as(Regression_shpData, 'Spatial')  #These analyses are easier to do when the data are of the SpatialPolygonsDataFrame class
class (shps)

# Bandwidth selection (optimal based on adaptive)
bw<-gwr.sel(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, 
            data=shps,
            method = "aic",
            adapt = TRUE)
bw
```

```{r gwr-fixed-bandwidth}
#| echo: false
#| eval: false
bw_fixed<-gwr.sel(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, 
            data=shps,
            method = "aic",
            adapt = FALSE)
bw_fixed
```

```{r gwr-with-adaptive1}
#| echo: false
#| eval: false

gwrmodel<-gwr(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV,
              data=shps,
              adapt = bw, #adaptive bandwidth determined by proportion of observations accounted for
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel
```

```{r gwr-with-fixed}
#| echo: false
#| eval: false

gwrmodel_fixed<-gwr(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV,
              data=shps,
              bandwidth = bw_fixed, #fixed bandwidth
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel_fixed
```

```{r map-local-r21}
#| echo: false
#| eval: false

localR2 <- gwrmodel$SDF$localR2
localR2_sf <- st_as_sf(gwrmodel$SDF)

ggplot(localR2_sf) +
  geom_sf(aes(fill = localR2), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Local R²", limits = c(0, 1)) +
  labs(
    title = "Map of Local R² Values (GWR)",
  ) +
  theme_void() +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.title = element_text(face = "bold")
  )
```

```{r gwr-morans}
#| echo: false
#| eval: false

gwr_res <- gwrmodel$SDF$gwr.e

moran.plot(gwr_res, queenlist)

moran.mc(gwr_res, queenlist, 999, alternative="two.sided")

```

```{r gwr-coefficient-mapping}
#| echo: false
#| eval: false

# --- 1. Extract GWR results ---
gwrresults <- as.data.frame(gwrmodel$SDF)

# --- 2. Convert your SpatialPolygonsDataFrame to sf ---
shps_sf <- st_as_sf(shps)

# --- 3. Compute standardized coefficients (coef / se) ---
shps_sf$coefPCTVACANT_st <- gwrresults$PCTVACANT / gwrresults$PCTVACANT_se
shps_sf$coefPCTSINGLES_st <- gwrresults$PCTSINGLES / gwrresults$PCTSINGLES_se
shps_sf$coefPCTBACHMOR_st <- gwrresults$PCTBACHMOR / gwrresults$PCTBACHMOR_se
shps_sf$coefLNNBELPOV_st <- gwrresults$LNNBELPOV / gwrresults$LNNBELPOV_se

# --- 4. Categorize coefficients according to your instructions ---
categorize_ratio <- function(x){
  cut(
    x,
    breaks = c(-Inf, -2, 0, 2, Inf),
    labels = c("dark red (< -2)", "pink (-2 to 0)", "light blue (0 to 2)", "dark blue (> 2)")
  )
}

shps_sf$coefPCTVACANT_cat <- categorize_ratio(shps_sf$coefPCTVACANT_st)
shps_sf$coefPCTSINGLES_cat <- categorize_ratio(shps_sf$coefPCTSINGLES_st)
shps_sf$coefPCTBACHMOR_cat <- categorize_ratio(shps_sf$coefPCTBACHMOR_st)
shps_sf$coefLNNBELPOV_cat <- categorize_ratio(shps_sf$coefLNNBELPOV_st)

# --- 5. Define manual colors ---
ratio_colors <- c(
  "dark red (< -2)" = "darkred",
  "pink (-2 to 0)" = "pink",
  "light blue (0 to 2)" = "lightblue",
  "dark blue (> 2)" = "darkblue"
)

# --- 6. Plot all predictors (example for PCTVACANT) ---
ggplot(shps_sf) +
  geom_sf(aes(fill = coefPCTVACANT_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (PCTVACANT)") +
  labs(title = "Standardized GWR Coefficients for PCTVACANT") +
  theme_minimal() +
  theme(legend.position = "right")

ggplot(shps_sf) +
  geom_sf(aes(fill = coefPCTSINGLES_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (PCTSINGLES)") +
  labs(title = "Standardized GWR Coefficients for PCTSINGLES") +
  theme_minimal() +
  theme(legend.position = "right")

ggplot(shps_sf) +
  geom_sf(aes(fill = coefPCTBACHMOR_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (PCTBACHMOR)") +
  labs(title = "Standardized GWR Coefficients for PCTBACHMOR") +
  theme_minimal() +
  theme(legend.position = "right")

ggplot(shps_sf) +
  geom_sf(aes(fill = coefLNNBELPOV_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (LNNBELPOV)") +
  labs(title = "Standardized GWR Coefficients for LNNBELPOV") +
  theme_minimal() +
  theme(legend.position = "right")

```

# Introduction (Google Doc at end)


# Methods

## The Concept of Spatial Autocorrelation
Spatial autocorrelation builds upon the 1st Law of Geography, Tobler’s Law, which states that all things are related but near things are more related than further observations. 

Spatial autocorrelation examines the relationship of values of the same variable at nearby locations. A positive relationship is indicated by related values at nearby locations and a negative relationship is indicated by significantly different values at nearby locations. Moran’s I is the statistical measure used to detect spatial autocorrelation, giving a value closer to 1 for positive spatially dependent values, -1 for negative spatially dependent values, and 0 for values that exhibit no spatial autocorrelation. The formula for calculating Moran’s I is as follows:
$$
I = \frac{N}{W} \cdot\frac{\sum_{i=1}^N \sum_{j=1}^N W_{ij} (X_i - \bar{X})(X_j - \bar{X})} {\sum_{i=1}^N (X_i - \bar{X})^2}
$$

$\bar{X}$  is the mean of the variable X, $X_i$ is the variable value at a particular location i, $X_j$ is the variable value at another location $j$, $W_{ij}$ is the value given by the weight matrix of location of $i$ relative to $j$, and n is the number of observations. This formula captures the covariance between neighboring values, standardized by the overall variance. For each pair of locations $i$ and $j$, Moran’s I measures how similarly their values deviate from the mean, multiplies those deviations, weights the result by their spatial proximity, and normalizes by the overall variance.

This analysis uses a queen contiguity weight matrix, which defines neighbors as polygons sharing either a border or a vertex. The matrix is square, n x n, where n is the number of observations, that assigns a value of 1 for neighbors and 0 otherwise. Unless there is a strong theoretical reason for using a particular weight matrix, statisticians will sometimes use multiple weight matrices (other contiguity-based measurements like rook or distance-based measurements) to ensure results are robust rather than just a byproduct of a single spatial definition.

In R, we can assess the statistical significance of Moran’s I using a Monte-Carlo permutation test  via the `moran.mc()` function. This method computes Moran’s I for the original variable, then randomly shuffles the variable values 999 times, recalculating Moran’s I for each permutation. We evaluate where our original Moran’s I falls by ranking it relative to the randomly permuted Moran’s I,  either in descending order for positive autocorrelation or ascending for negative. A pseudo p-value is then calculated by dividing the rank of the original Moran’s I by the total number of permutations, estimating the likelihood of observing such a value under spatial randomness. We test for the null hypothesis, $H_0$, no spatial autocorrelation against the two-sided alternative $H_a$, positive spatial autocorrelation or negative spatial autocorrelation. Visual diagnostics include a histogram of the permuted data’s Moran’s I values with the original Moran’s I highlighted, and a Moran scatterplot comparing the block groups’ original variable values to the average of its neighbors, also known as spatially lagged variable values. A clear pattern in the scatterplot would suggest spatial autocorrelation while randomness implies no spatial autocorrelation.

Local spatial autocorrelation examines how similar or dissimilar values at one location are to nearby locations. Rather than describing the overall global spatial patterns, local spatial autocorrelation pinpoints areas of spatial clustering or spatial outliers. We test for local spatial autocorrelation using LISA, Local Indices of Spatial Association. In this case, we use the local Moran’s I as a statistical measure of local spatial autocorrelation. In R, we compute Local Moran’s I using the `localmoran()` function. Conceptually, the statistic is calculated by taking the deviation of a value at location $i$ from the global mean, multiplying it by the weighted average of its neighbors’ deviations, and normalizing by the total variance across all locations. The null hypothesis for local spatial autocorrelation is $H_0$, no local spatial autocorrelation at location $i$ while the two-sided alternative hypothesis is $H_a$, a positive or negative spatial autocorrelation at location i. The `localmoran()` function implements a permutation-based test for statistical significance. For each location, the value at $i$ is held constant while the values of its neighbors are randomly shuffled. A two-sided pseudo p-value is then computed based on the rank of the original $Ii$ relative to the permuted values’ Moran’s I. This p-value is returned by indexing the results of the `localmoran` function with $\Pr(z \neq E(I_i))$ and can be conceptualized as the probability that the observed $Ii$ is significantly different from what we’d expect under spatial randomness in either direction (positive or negative). Visually, we can create a map that shows the spatial distribution of Local Moran’s I p-values and clusters to further assess for local spatial autocorrelation. 

## A Review of OLS Regression and Assumptions

Ordinary Least Squares (OLS) regression estimates the relationship between a dependent variable and one or more independent variables by minimizing the sum of squared differences between observed and predicted values. In the context of multiple regression, OLS quantifies the unique contribution of each predictor to the outcome while controlling for the influence of the others. Unlike simple regression, which models the dependent variable using a single predictor, multiple regression incorporates several predictors, each with a coefficient representing its effect on the dependent variable. Multiple regression relies on several key assumptions, most of which mirror the assumptions of simple regression: linearity between the dependent variable and each predictor, normally distributed residuals, randomness of residuals— indicating that observations are not systematically related, homoscedastic of residuals or constant variance across all values, a continuous dependent variable, and, a unique assumption for multiple regression, no perfect multicollinearity between predictors. A more comprehensive overview of Ordinary Least Squares (OLS) regression can be found in Homework 1: *Using OLS Regression to Predict Median House Values in Philadelphia*.

As previously mentioned, a core assumption of OLS regression is the randomness of residuals or, in other words, the independence of residuals from one another. When spatial autocorrelation is present, this indicates that values of a variable at nearby locations are related to one another, violating the assumption of independence. As a result, the OLS error term may contain insightful spatial patterns rather than random noise and beta coefficients may be inefficient estimates. In practice, this can manifest as systematic over- or under-prediction. We can statistically quantify the spatial autocorrelation of residuals by calculating the Moran’s I of the OLS residuals, Moran’s I which we previously introduced as a measure of bidirectional spatial dependence.

Another way to assess spatial dependence in OLS residuals is to regress each residual on its spatially lagged counterpart. A spatial lag refers to the value of a variable at neighboring locations, which, in this case, are defined by the queen weights matrix. By creating spatially lagged residuals and regressing each residual on its lagged value, we can test whether residuals are systematically related across space. In the statistical summary of this regression, the slope $b$ represents the coefficient of the lagged residuals when predicting the original residuals. This slope quantifies the strength and direction of spatial dependence. If it is significantly different from zero, it suggests that residuals are spatially autocorrelated, meaning the assumption of independence is violated and the OLS estimates may be compromised. Visually, this relationship can be assessed using a scatterplot of OLS residuals against the weight or spatially lagged residuals.

In addition to estimating regression coefficients, we can use various libraries in R to perform statistical tests that assess other assumptions of OLS. One assumption we can test for is homoscedasticity, which refers to the constant variance of residuals across all predicted values. If residuals vary systematically with predicted values, this indicates heteroscedasticity, or non-constant variance, which violates the assumption. This assumption is closely tied to the independence of errors as residuals that show heteroscedasticity could imply non-random or dependent variance which can compromise estimation. A simple visual diagnostic involves plotting OLS residuals by predicted residuals. In R, we can use the `whitestrip` and `lmtest` library to perform three commonly used statistical tests: the Breusch-Pagan Test, the Koenker-Bassett Test or studentized Breusch-Pagan Test, and the White Test. Each test evaluates the null hypothesis of homoscedasticity or no heteroscedasticity and the alternative hypothesis of heteroscedasticity.  If the result p-value for these tests is less than 0.05, then we can reject the null hypothesis for the alternate hypothesis of heteroscedasticity.

Another key assumption of OLS regression is the normality of errors. Residuals should behave like random noise, containing no systematic structure, and should follow a normal distribution. A simple visual diagnostic of residual normality is to plot residuals using a histogram. In R, we can perform a statistical diagnostic of normality by using the Jarque-Bera Test, available through the `tseries` package. The null hypothesis for the Jarque-Bera Test is that the residuals follow a normal distribution while the alternative hypothesis is non-normality or non-normal distribution. If the resulting p-value is less than 0.05, we reject the null hypothesis in favor of the alternative, indicating a violation of the normality assumption.

## Spatial Lag and Spatial Error Regression

The statistical programming language R was used to run spatial lag and spatial error regressions. 

The spatial lag regression model operates on the assumption that a dependent variable's value at any given location is associated by the values of that same variable at neighboring locations. To define spatial proximity in this analysis, we utilized a queen contiguity weight matrix W, which establishes neighboring relationships between any locations that share a border or vertex. The model equation for our spatial lag regression model is as follows:
$$
\begin{aligned}
\text{LNMEDHVAL} = &\rho W(\text{LNMEDHVAL}) + \beta_0 + \beta_1 \text{PCTVACANT} \\
&+ \beta_2 \text{PCTSINGLES} + \beta_3 \text{PCTBACHMOR} + \beta_4 \text{LNNBELPOV} + \varepsilon
\end{aligned}
$$
Here, $\rho$ is the coefficient of the y-lag variable W(LNMEDHVAL), which itself is the weighted average of log-transformed median house value at neighboring locations.  $\varepsilon$ represents the random variation not included in the model. The $\beta$ coefficients of the spatial lag models have a fairly complex interpretation which is beyond the scope of the report.

In contrast, the spatial error regression model relies on the assumption that the residual at one location is associated with residuals at nearby locations. As in the previously discussed spatial lag model, we will use a queen contiguity weight matrix W. The spatial error regression model is as follows:
$$
\begin{aligned}
\text{LNMEDHVAL} = &\beta_0 + \beta_1 \text{PCTVACANT} + \beta_2 \text{PCTSINGLES} \\
&+ \beta_3 \text{PCTBACHMOR} + \beta_4 \text{LNNBELPOV} + \lambda W(\varepsilon) + u
\end{aligned}
$$
Similar to OLS regression, $\beta_0$ represents the dependent variable when all predictors are zero. Also, the coefficients of the predictors $\beta_1, \beta_2, \beta_3, \beta_4$ each represent the change in the dependent variable with a one unit increase in their respective predictor, holding all other predictors constant. Uniquely, the error term is decomposed into a component with a spatial pattern $\lambda W(\varepsilon)$ and a random component $u$, where $\lambda$ is constrained between -1 and 1 and measures the strength of spatial dependence in the residuals.

It is important to note that all of the previously discussed assumptions of OLS regression are required for both spatial error and spatial lag models, with the exception of independence of observations. Both spatial models are specifically designed to address violations of this independence assumption by accounting for spatial autocorrelation. The goal if for these models to produce residuals free from spatial patterns that would otherwise indicate systematic over- or under-prediction.

The performance of both spatial models will be compared to that of the OLS regression from the previous report using multiple criteria. The first is the Akaike Information Criterion (AIC), which is an estimator of predictor error and provides insight into the quality of the model by penalizing increasing number of predictors that could lead to over-fitting. The lower the AIC, the better the fit of the model.

The second criterion is Log Likelihood, which measures model fit based on maximum likelihood estimation. A higher Log Likelihood indicates better model fit, though this metric can only compare nested models—models where removing parameters from one yields another. Both spatial lag and spatial error models are nested within OLS regression due to their additional spatial parameters, making them comparable to OLS but not to each other using Log Likelihood.

A third criterion is the Likelihood Ratio Test, which also comes with the restriction that it be used between nested models (i.e comparing OLS to a spatial model). The null hypothesis of this test is as follows:
$$
H_0: \text{The spatial model is not a better specification than the OLS model.} 
$$
When this test is conducted, if p < 0.05, we can reject $H_0$ and state that the spatial model is doing better than the OLS model.

Finally, the Moran's I statistic of the regression residuals can be compared across models. A Moran's I value near zero indicates that the model has successfully accounted for spatial autocorrelation. When comparing spatial models to OLS, insignificant values closer to zero suggest better correction of spatial dependence.

## Geographically Weighted Regression
Geographically Weighted Regression (GWR) is a spatial modeling technique that extends the conventional Ordinary Least Squares (OLS) regression by allowing model parameters to vary across geographic space.
Whereas OLS assumes a single global relationship between the dependent and independent variables, GWR accounts for spatial nonstationarity, meaning that the strength and direction of these relationships may differ by location.

### Mathematical Formulation

The general form of the OLS model is:
$$
y_i = \beta_0 + \sum_{k=1}^{p} \beta_k x_{ik} + \varepsilon_i
$$

where:

- $y_i$ is the dependent variable at observation $i$  
- $x_{ik}$ is the value of predictor $k$ at observation $i$  
- $\beta_k$ are the global regression coefficients  
- $\varepsilon_i$ is the random error term assumed to be normally distributed and independent 

GWR extends this model by allowing each coefficient to vary across geographic space:
$$
y_i = \beta_0(u_i, v_i) + \sum_{k=1}^{p}\beta_k(u_i, v_i)x_{ik} + \varepsilon_i
$$

where $(u_i, v_i)$ are the spatial coordinates of observation $i$, and $\beta_k(u_i, v_i)$ represents the **local coefficient** for predictor $k$ at that location.  
The model estimates parameters at each location using **weighted least squares**, with weights determined by a spatial kernel function.  
The local coefficients are estimated as:

$$
\hat{\boldsymbol{\beta}}(u_i,v_i) = (X^{T}W_iX)^{-1} X^{T}W_i y
$$

where:

- $X$ is the matrix of independent variables  
- $y$ is the vector of dependent variables  
- $W_i$ is a diagonal matrix of spatial weights for location $i$  

#### Global R² (Goodness of Fit)

Similar to OLS regression, GWR reports a global R-squared statistic that measures the overall proportion of variance in the dependent variable explained by the model. Its value ranges from 0 to 1, with higher values indicating better model fit.

The formula for the coefficient of determination is:

$$
R^2 = 1 - \frac{SSE}{SST} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y_i})^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}
$$

where:
- $y_i$ = observed values,  
- $\hat{y_i}$ = predicted values from the model,  
- $\bar{y}$ = mean of observed values,  
- $SSE$ = sum of squared errors,  
- $SST$ = total sum of squares.

#### Bandwidth Selection and Weighting Scheme

In Geographically Weighted Regression (GWR), the bandwidth ($h$) determines the spatial extent over which data points influence each local regression. Two main approaches are commonly used: **fixed bandwidth** and **adaptive bandwidth**.

##### Fixed Bandwidth

Under a fixed bandwidth, the spatial extent $h$ remains constant across all regression points, meaning the geographic area of influence is the same, while the number of observations within each local regression may vary.

$$
w_{ij} =
\begin{cases}
e^{-0.5 \left( \dfrac{distance_{ij}}{h} \right)^2}, & \text{if } distance_{ij} \le h \\
0, & \text{otherwise}
\end{cases}
$$

where $distance_{ij}$ is the distance between observation $i$ and $j$.  
This form is known as a *Gaussian kernel*, which gives higher weight to nearer observations.

##### Adaptive Bandwidth

Under an adaptive bandwidth, the number of nearest neighbors ($N$) is fixed, but the geographic area $h$ varies according to local data density.  
Areas with dense data use smaller $h$, while sparse regions require larger $h$ to include enough neighbors.

$$
w_{ij} =
\begin{cases}
\left[ 1 - \left( \dfrac{distance_{ij}}{h} \right)^2 \right]^2, & \text{if } j \text{ is one of } i\text{’s } N \text{ nearest neighbors} \\
0, & \text{otherwise}
\end{cases}
$$

In this case, $h$ adapts to ensure that each local regression includes approximately the same number of observations.  
For example, if $N = 20$, one area might require $h = 5000\,ft$ to reach its 20th neighbor, while another only needs $h = 2500\,ft$.

In this analysis, an **adaptive Gaussian kernel** was used, ensuring that dense urban areas and sparse suburban areas are both modeled effectively.  
The optimal bandwidth was chosen by minimizing the **Akaike Information Criterion (AIC)**, which balances model fit and complexity.

To test model robustness, a fixed-bandwidth GWR was also calibrated for comparison.


### Model Diagnostics

Model performance was assessed using both global and local indicators:

- **Global diagnostics:** Akaike Information Criterion (AIC, AICc) and quasi-global $R^2$  
- **Local diagnostics:** Local $R^2$, coefficient maps, and standardized ratios ($\beta / SE$)  
- **Spatial dependence:** Moran’s I statistic of residuals tested whether the GWR residuals remained spatially autocorrelated  

Together, these diagnostics determine whether GWR improves explanatory power over OLS and whether it effectively removes spatial dependence from model residuals.


# Results

## Spatial Autocorrelation
**Random Permutation Test/ Monte-Carlo Simulation Table of LNMEDHVAL**
```{r global Morans I for LNMEDHVAL permuatation table}
#| echo: false
moranMC<-moran.mc(Regression_shpData$LNMEDHVAL, queenlist, nsim=999, alternative="two.sided")  #We use 999 permutations

moranMC

```
Our global Moran’s I value, 0.793, of our dependent variable was considerably different from 0, indicating high positive spatial autocorrelation. Our random permutation test suggests that our Moran’s I was statistically significant as it returned a p-value of less than 0.00000000000000022 which falls into the statistically significant threshold (p < 0.05).The rank of our observed Moran’s I further enforces our findings, suggesting that none of the 999 randomly permuted simulations produced a value as extreme as the one observed. Thus, we can reject the Moran’s I null hypothesis of no spatial autocorrelation as well as consider LNMEDHVAL to be significantly spatially autocorrelated.  
```{r global Morans I for LNMEDHVAL permutation histogram}
#| echo: false
moranMCres<-moranMC$res
hist(moranMCres, freq=10000000, nclass=100,
main = "Distribution of Moran\'s I Values From Logged Median House Value\n Permutation Tests\n Red Line = Observed Moran's I",
xlab = "Moran\'s I")   #Draws distribution of Moran's I's calculated from randomly permuted values
# Here, we draw a red vertical line at the observed value of our Moran's I
abline(v=moran(Regression_shpData$LNMEDHVAL, queenlist, n=length(queenlist$neighbours), S0=Szero(queenlist))$`I`, col='red')  
```
Visually, the histogram of the distribution of global Moran’s I values from all the permutation tests shows that our original Moran’s I (highlighted by the red vertical line) towers far above the other Moran’s I. This histogram further indicates that the possibility of retaining our original Moran’s I under true spatial randomness is low. 
```{r retrieve p-values and generate clusters for LISA maps, include=FALSE}
#| echo: false

#Obtaining the Local Moran's P-Values (two-sided)
Regression_shpData$lmp <- lmoran[, "Pr(z != E(Ii))"]

#Create LISA clusters 
mp <- moran.plot(as.vector(scale(Regression_shpData$LNMEDHVAL)), queenlist)

```

```{r LISA p-value and cluster map, fig.width=6, fig.height=8}
#| echo: false

Regression_shpData <- st_make_valid(Regression_shpData) #Sometimes necessary if projection is off

Regression_shpData$quadrant <- NA
# high-high
Regression_shpData[(mp$x >= 0 & mp$wx >= 0) & (Regression_shpData$lmp <= 0.05), "quadrant"]<- 1
# low-low
Regression_shpData[(mp$x <= 0 & mp$wx <= 0) & (Regression_shpData$lmp <= 0.05), "quadrant"]<- 2
# high-low
Regression_shpData[(mp$x >= 0 & mp$wx <= 0) & (Regression_shpData$lmp <= 0.05), "quadrant"]<- 3
# low-high
Regression_shpData[(mp$x <= 0 & mp$wx >= 0) & (Regression_shpData$lmp <= 0.05), "quadrant"]<- 4
# non-significant
Regression_shpData[(Regression_shpData$lmp > 0.05), "quadrant"] <- 5

#create p-value categories for graphing 
Regression_shpData$pval_cat <- cut(
  Regression_shpData$lmp,
  breaks = c(-Inf, 0.001, 0.01, 0.05, Inf),
  labels = c("< 0.001", "< 0.01", "< 0.05", "0.05 or more"),
  include.lowest = TRUE
)

# plot for p-values
ggplot(Regression_shpData) +
  geom_sf(aes(fill = pval_cat), color = "grey50", alpha = 0.8) +
  scale_fill_manual(
    values = c("< 0.001" = "darkblue", "< 0.01" = "blue", "< 0.05" = "lightblue", "0.05 or more" = "white"),
    name = NULL
  ) +
  labs(title = "LISA P-Value Map") +
  theme_void() +
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold"),
    panel.border = element_blank()
  )

#create cluster categories for graphing
Regression_shpData$cluster_cat <- factor(
  Regression_shpData$quadrant,
  levels = c(1, 2, 3, 4, 5),
  labels = c("High-High", "Low-Low", "High-Low", "Low-High", "Non-significant")
)

# plot clusters 
ggplot(Regression_shpData) +
  geom_sf(aes(fill = cluster_cat), color = "grey50", alpha = 0.8) +
  scale_fill_manual(
    values = c("High-High" = "red", "Low-Low" = "blue", "High-Low" = "lightpink", "Low-High" = "skyblue2", "Non-significant" = "white"),
    name = NULL
  ) +
  labs(title = "LISA Cluster Map") +
  theme_void()+
  theme(
    legend.position = "right",
    legend.text = element_text(size = 10),
    plot.title = element_text(size = 14, face = "bold"),
    panel.border = element_blank())




```
The Local Moran’s I analysis was conducted to identify spatial clusters and outliers in the distribution of LNMEDHVAL across Philadelphia. Two maps were generated: a significance (p-value) map that highlights areas where spatial autocorrelation is statistically significant and a cluster map that classifies areas into High-High, Low-Low, High-Low, Low-High, and Not Significant clusters based on local spatial relationships.A majority of North East Philadelphia, Upper North Philadelphia and Center City as well as parts of West Philadelphia such as Wynnefield and University City exhibited high values surrounded by other high-value neighbors. A majority of North Philadelphia, Parkside in West Philadelphia, Kingsessing in Southwest Philadelphia, and parts of the South Philadelphia neighborhood exhibited Low-Low relationships or low values surrounded by other low-value neighbors.
Only a few areas exhibited Low-High spatial relationships or low values surrounded by other low value neighbors: parts of the South Philadelphia neighborhood and one block group in Torresdale of North East Philadelphia. The Significance Map confirms that most of the identified clusters fall within statistically significant zones (p < 0.05) while the rest of Philadelphia was rendered Not Significant. 
## A Review of OLS Regression and Assumptions: Results

```{r ols summary table}
#| label: tbl-ols
#| echo: false
#| tbl-pos: "H"
#| tbl-cap: "OLS Regression Summary"
#| message: false  # Hide messages
#| warning: false  # Hide warnings

Regression_shpData$predvals <- fitted(reg1) 

Regression_shpData$resids <- residuals(reg1)

Regression_shpData$stdres <- rstandard(reg1)

reg1 <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=Regression_shpData)

summary(reg1)
```
Our OLS results considered all of our model’s predictors, PCTBACHMOR, LNNBELOPOV, PCTSINGLES, and PCTVACANT to be significant and returned an $R^2$ of 0.6623 meaning that approximately 66% of the variance in logged median house values could be explained by the model.
```{r OLS predvals by std residual scatter plot}
#| echo: false
plot(Regression_shpData$predvals, Regression_shpData$stdres,
     main = "Standardized Residuals By Predicted Values",
     xlab = "Predicted Values",
     ylab = "Standardized Residuals")
```
In our initial visual test for heterodasticity in HW 1, we concluded that our scatter plot
of our standardized residuals showed general homoscedasticity or consistent variance of residuals. We decided there was general uniformity of the standardized residuals as most were between -2 and positive 2. There were some outliers that extend past -4 and 4 but determined they did not dominate the overall pattern. We also observed no funneling affect or any other pattern of non-constant variance. 
**Breusch-Pagan Test Results**
```{r Breusch-Pagan Test results}
#| echo: false
bptest(reg1, studentize=FALSE)

```
The p-value from the Breusch-Pagan test suggests, however, that the residuals in the OLS regression model likely exhibit heteroscedasticity. The Breusch-Pagan test evaluates whether the residuals from a regression model exhibit constant variance. Since the resulting p-value falls below the conventional significance threshold of 0.05, we reject the null hypothesis of homoscedasticity in favor of the alternative hypothesis that the residuals have non-constant variance.
**Studentized Breusch-Pagan Test Results**
```{r Studentized Breusch-Pagan Test results}
#| echo: false
bptest(reg1)   

```
The p-value from the studentized Breusch-Pagan test, a more robust version of our initial Breysch-Pagan test, also suggests heteroscedasticity since the p-value is less than 0.00000001102, well below the conventional threshold of 0.05, allowing us to reject the null hypothesis of homoscedasticity. 
**White Test Results**
```{r White Test results}
#| echo: false
white_test(reg1)

```
The White’s test is another statistical assessment we used to test whether the residuals from an OLS regression model exhibit constant variance. The results show that p-value from this final statistical test is effectively zero, providing strong evidence against the null hypothesis of homoscedasticity. This result reinforces the presence of heteroscedasticity in the model. All three statistical measures heteroscedasticity  contradict our initial visual assessment, which did not clearly indicate a violation. The discrepancy highlights that visual diagnostics alone may be insufficient for detecting non-constant variance and emphasizes the importance of formal statistical testing in validating model assumptions.
```{r Historgram of OLS std res}
#| echo: false

hist(Regression_shpData$stdres,
     main = "Histogram of Standardized Regression Residuals",
     xlab = "Standardized Residuals",
     ylab = "Frequency")
```
In our initial visual assessment for the assumption of normally distributed residuals, we concluded that the  histogram of the standardized residuals showed the normality in residuals needed per our assumption and supported the need for the logarithmic transformations we performed to achieve normality.
**Jarque Bera Test Results**
```{r Jarque Bera Test results}
#| echo: false
jarque.bera.test(reg1$residuals)


```
The Jarque-Bera test statistically evaluates whether the residuals from a regression model follow a normal distribution. In this case, the p-value is less than 0.00000000000000022, well below the conventional threshold of 0.05, allowing us to reject the null hypothesis that the residuals are normally distributed. This contrast between the visual and statistical diagnostic for normality further reinforces the need for formal assessments of regression assumptions.
```{r plot of spatially-lagged-residuals}
#| echo: false
wt_residu <- sapply(queen, function(x) mean(stdres[x])) #created weighted residuals 

plot(wt_residu, stdres,
main="Standardized OLS Residuals by\n Weighted (Spatially Lagged) Residuals",
xlab="Weight Residuals",
ylab="Standardized OLS Residuals") #weight residuals by ols residuals 

```
The scatter plot between the OLS residuals and their spatially lagged counterparts has a discernible linear trend, suggesting that residuals are not randomly distributed in space and, instead, exhibit spatial dependence.
**Summary of Weight Residual Regressed on OLS Residuals**
```{r summary of weighted residuals regressed on ols residuals}
#| echo: false
#Note the beta coefficient of the wt_residu. 0.73235. This suggests that there is spatial autocorrelation (areas with high residuals are near similar areas)

res.lm <- lm(formula=stdres ~ wt_residu) #regressed weighted residuals by ols residuals 

summary(res.lm)
```
The $b$ coefficient of 0.73235 from the summary table of the weighted residuals regressed on the OLS residuals quantifies this relationship and indicates a strong positive association between each residual and the average residuals of its spatial neighbors.
```{r Moran\'s I plot of OLS residuals }
#| echo: false
moran.plot(stdres, queenlist,
           main="Moran Plot of Spatially Lagged Residuals by Standardized\n OLS Residuals",
           xlab="Standardized OLS Residuals",
           ylab="Spatially Lagged Residuals")

```
In the Moran scatter plot of the spatially lagged residuals by the OLS residual, the clustering of values along the diagonal line indicates a clear pattern of spatial dependence, suggesting that residuals at one location tend to resemble those of neighboring locations.
**Random Permutation Test/ Monte-Carlo Simulation Table for OLS Regression**
```{r Moran\'s I results of OLS residuals}
#| echo: false
#Note the small p-value of Moran's I. Same spatial autocorrelation in the scatterplot.
moran.mc(stdres, queenlist, 999, alternative="two.sided") #OLS residuals Moran's I and permutations 

```
The observed Moran’s I value, 0.3124, indicates moderate positive spatial autocorrelation. Out of 1000 permutations, our observed statistic was the most extreme, meaning none of the randomized simulations produced a Moran’s I as large. Our p-value of less than 0.00000000000000022 was much less than the statistical threshold, indicating our observed Moran’s I was statistically significant.

Thus, the Moran’s I and the $b$ coefficient from the regression of spatially lagged residuals on OLS residuals both reinforce the presence of spatial autocorrelation. Together, these diagnostics consistently point to the violation of the OLS assumptions of independent errors the need for spatial modeling. Specifically, these findings motivate us to use the spatial error and spatial lag regression models, which explicitly account for spatial dependence.

## Spatial Lag and Spatial Error Regression Results

### Spatial Lag Regression Results
The results of the Spatial Lag model are presented in Table 2 below.

```{r spatial-lag-reg-results}
#| label: tbl-spatial-lag
#| echo: false
#| tbl-pos: "H"
#| tbl-cap: "Spatial Lag Model Results"

lagreg <- lagsarlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, 
                   data = Regression_shpData, queenlist)
summary(lagreg)
```
The $\rho$ (rho) term, the spatial lag term, measures the influence of neighboring block group LNMEDHVAL on a given block's LNMEDHVAL. According to the results, this coefficient is 0.6511 and is very statistically significant with a p-value far less than our threshold of 0.05. This coefficient suggests that for a 1% increase in nearby block group house values, we can expect a 0.65% increase in a given block group's log-transformed Median House Value.

All four predictors (PCTVACANT, PCTSINGLES, PCTBACHMOR and LNNBELPOV) are statistically significant in the spatial lag model (all p-values are far less than 0.05). This indicates that each predictor has a meaningful relationship with log-transformed Median House Value even after accounting for spatial dependence. In particular, PCTVACANT and LNNBELPOV are associated with lower housing values, while PCTSINGLES and PCTBACHMOR are associated with higher values.

We can now compare these results to that of the OLS model from the prior report. The direction and significance of the predictors are consistent, though the spatial lag model yields smaller coefficients, suggesting that part of the variation previously attributed to these predictors is actually explained by the spatial lag term's inclusion of neighboring block group effects.

Below are the results of a Breusch-Pagan test on the Spatial Lag regression model.
```{r saptial-lag-bp-results}
#| echo: false
#| results: asis
cat("\\begin{center}\n")
print(bptest.Sarlm(lagreg, studentize = FALSE))
cat("\\end{center}\n")
```
Based on these results of the p-value far less than 0.05, we reject the null hypothesis of homoscedasticity. This indicates that even after accounting for spatial dependence in the spatial lag model, the residuals still exhibit heteroscedasticity—in other words, the variance of the errors is not constant across observations.

The spatial lag model fits the data substantially better than OLS, with a much lower AIC (525.48 vs. 1435), higher log-likelihood, and a highly significant Likelihood Ratio test (911.51, p-value far less than 0.05), indicating that accounting for spatial dependence improves model performance. However, the Breusch–Pagan test shows that residual heteroscedasticity remains, suggesting that further adjustments are needed.

To further investigate the spatial lag model's improvement over OLS, we can examine the Moran's I scatter plot of spatial lag regression residuals and a Monte-Carlo simulation of Moran's I.
```{r spatial-lag-morans-results}
#| echo: false
lagreg_res<-lagreg$residuals

lagMoranMc<-moran.mc(lagreg_res, queenlist,999, alternative="two.sided")
lagMoranMc

moran.plot(lagreg_res, queenlist)
```
Here, we see that the residuals from the Spatial Lag Model aren't significantly spatially auto correlated. Some weak negative autocorrelation remains as evidenced by the scatter plot, but overall the model has largely mitigated the spatial clustering present in OLS residuals.

Overall, with some room for improvement, the Spatial Lag Model appears to perform better than the OLS regression mode.

### Spatial Error Regression Results
The results of the Spatial Lag model are presented in Table 3 below.
```{r spatial-error-reg-results}
#| label: tbl-spatial-err
#| echo: false
#| tbl-pos: "H"
#| tbl-cap: "Spatial Error Model Results"

errreg<-errorsarlm(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=Regression_shpData, queenlist)
summary(errreg)
```
As mentioned previously, the $\lambda$ (lambda) term measures the strength of spatial dependence in the residuals. According to the results, this coefficient is 0.8149 and is very statistically significant with a p-value far less than our threshold of 0.05. This coefficient indicates that approximately 81% of the spatial pattern in the residuals can be attributed to effects from neighboring block groups' residuals. In other words, after accounting for the predictors, unmeasured factors affecting log-transformed Median House Values in one block group are strongly correlated with those same unmeasured factors in neighboring block groups.

All four of the predictors (PCTVACANT, PCTSINGLES, PCTBACHMOR and LNNBELPOV) are also statistically significant in the Spatial Error model, all with p-values far less than 0.05. As mentioned in the methods section, interpreting the beta coefficients of Spatial Error Models is beyond the scope of this report.

We can compare the results of the Spatial Error model to that of the OLS model from the previous report. There are similar negative relationships between LNMEDHVAL and predictors PCTVACANT and LNNBELBOV, and positive relationships between LNMEDHVAL and predictors PCTSINGLES and PCTBACHMOR. Similar to the results in the Spatial Lag section, here the coefficients are smaller than that in the OLS model, suggesting that the Spatial Error term accounts for variation missing from the OLS model.

Below are the results of a Breusch-Pagan test on the Spatial Error regression model. 
```{r saptial-err-bp-results}
#| echo: false
#| results: asis
cat("\\begin{center}\n")
print(bptest.Sarlm(errreg, studentize = FALSE))
cat("\\end{center}\n")
```
With a p-value far less than 0.05 in the Breusch-Pagan test, we reject the null hypothesis of homoscedasticity. This shows that even for the Spatial Error model, the residuals still exhibit heteroscedasticity.

The Spatial Error model fits the data substantially better than OLS, with a much lower AIC (759.38 vs. 1435), higher log-likelihood, and a highly significant Likelihood Ratio test (677.61, p-value far less than 0.05), indicating that including the spatial error component improved model performance in comparison to OLS. Similar to the results from Spatial Lag, the results of the Breusch-Pagan test warrant further investigation to reduce heteroscedasticity.

To further investigate the Spatial Error model’s improvement over OLS, we can examine the Moran’s I scatter plot of spatial error regression residuals and a Monte-Carlo simulation of Moran’s I.
```{r spatial-error-morans-results}
#| echo: false
errreg_res <- residuals(errreg)

errMoranMc<-moran.mc(errreg_res, queenlist,999, alternative="two.sided")
errMoranMc

moran.plot(errreg_res, queenlist)
```
The residuals from the Spatial Error Model are not meaningfully spatially autocorrelated. The Moran’s I statistic of –0.0945 is near zero, and the scatter plot shows residuals with a very weak negative relationship. Although the test is statistically significant (p = 0.002), the near-zero Moran’s I value indicates that the model has effectively removed the spatial autocorrelation present in the OLS residuals.

Similar to the results from the Spatial Lag model, here we see the Spatial Error model perform better than OLS regression with room for improvement. Next, we can compare the two spatial models together.

### Comparing the Spatial Lag Model to the Spatial Error Model

The Spatial Lag Model has a substantially lower AIC (525.48 vs. 759.38), indicating it provides a better fit to the data. The difference of approximately 234 is quite large, strongly favoring the spatial lag model.
This suggests that the spatial dependence in log-transformed median home values is better captured by direct spillover effects from neighboring block group log-transformed median home values (the lag model) rather than by spatial error terms (the error model). In other words, a block group's log-transformed median home value is more directly influenced by the actual log-transformed median home values in surrounding block groups than by unmeasured spatially-structured factors.

## Geographically Weighted Regression Results
The geographically weighted regression (GWR) model was estimated to assess how the relationships between housing values and neighborhood characteristics vary across space. Both adaptive and fixed bandwidths were tested, and the model diagnostics and spatial patterns were examined to evaluate local model fit and remaining spatial dependence.

### Bandwidth Selection
1.Adaptive bandwith
```{r gwr-adaptive bandwidth-results}
#| echo: false

#Setting an adaptive bandwidth
shps <- as(Regression_shpData, 'Spatial')  #These analyses are easier to do when the data are of the SpatialPolygonsDataFrame class
class (shps)

# Bandwidth selection (optimal based on adaptive)
bw<-gwr.sel(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, 
            data=shps,
            method = "aic",
            adapt = TRUE)
bw
```
2.Fixed bandwith
```{r gwr-fixed-bandwidth-results}
#| echo: false

bw_fixed<-gwr.sel(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, 
            data=shps,
            method = "aic",
            adapt = FALSE)
bw_fixed
```
The adaptive bandwidth minimized the AIC and was selected for the final model. This approach adjusts to the spatial density of observations, allowing greater local flexibility in heterogeneous areas.

### Adaptive GWR Model
```{r gwr-with-adaptive-results}
#| echo: false

gwrmodel<-gwr(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV,
              data=shps,
              adapt = bw, #adaptive bandwidth determined by proportion of observations accounted for
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel
```

### Fixed Bandwidth Model (for comparison)
```{r gwr-with-fixed-results}
#| echo: false

gwrmodel_fixed<-gwr(formula=LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV,
              data=shps,
              bandwidth = bw_fixed, #fixed bandwidth
              gweight=gwr.Gauss,
              se.fit=TRUE, #to return local standard errors
              hatmatrix = TRUE)
gwrmodel_fixed
```
The adaptive GWR model substantially improved performance compared to the global OLS regression, with a lower AIC (308.7) and a higher quasi-global $R^2$ (0.848), indicating stronger explanatory power after accounting for spatial variation. In contrast, the fixed-bandwidth GWR model produced a higher AIC (352.3) and a lower quasi-global $R^2$ (0.844), confirming that adaptive bandwidth selection provides a better fit for spatially heterogeneous urban data.

### Local R² Mapping
```{r map-local-r2-results}
#| echo: false

localR2 <- gwrmodel$SDF$localR2
localR2_sf <- st_as_sf(gwrmodel$SDF)

ggplot(localR2_sf) +
  geom_sf(aes(fill = localR2), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Local R²", limits = c(0, 1)) +
  labs(
    title = "Map of Local R² Values (GWR)",
  ) +
  theme_void() +
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.title = element_text(face = "bold")
  )
```
The local $R^2$ values show substantial spatial variation across Philadelphia. The highest values are concentrated in Center City and University City, where the model explains up to nearly all of the variation in housing values. These areas correspond to dense, high-value neighborhoods with more consistent socioeconomic patterns, allowing the predictors—such as educational attainment and vacancy rate—to capture local dynamics effectively. In contrast, the outer neighborhoods and peripheral tracts, particularly in the Northeast and Northwest sections, exhibit lower local $R^2$ values, suggesting that housing prices there are influenced by additional unmeasured factors (e.g., land use mix, environmental amenities, or localized market conditions) not fully represented in the model.

### Moran’s I of GWR Residuals
```{r gwr-morans-results}
#| echo: false

gwr_res <- gwrmodel$SDF$gwr.e

moran.plot(gwr_res, queenlist)

moran.mc(gwr_res, queenlist, 999, alternative="two.sided")

```
The Moran’s I value for the GWR residuals (0.033, p = 0.016) indicates that most of the spatial autocorrelation present in the OLS residuals (Moran’s I ≈ 0.31, p < 0.001) has been substantially reduced. This suggests that the GWR model effectively accounts for spatial dependence by allowing regression coefficients to vary across space.

### Mapping of Local Coefficients
```{r gwr-coefficient-mapping-results}
#| echo: false

# --- 1. Extract GWR results ---
gwrresults <- as.data.frame(gwrmodel$SDF)

# --- 2. Convert your SpatialPolygonsDataFrame to sf ---
shps_sf <- st_as_sf(shps)

# --- 3. Compute standardized coefficients (coef / se) ---
shps_sf$coefPCTVACANT_st <- gwrresults$PCTVACANT / gwrresults$PCTVACANT_se
shps_sf$coefPCTSINGLES_st <- gwrresults$PCTSINGLES / gwrresults$PCTSINGLES_se
shps_sf$coefPCTBACHMOR_st <- gwrresults$PCTBACHMOR / gwrresults$PCTBACHMOR_se
shps_sf$coefLNNBELPOV_st <- gwrresults$LNNBELPOV / gwrresults$LNNBELPOV_se

# --- 4. Categorize coefficients according to your instructions ---
categorize_ratio <- function(x){
  cut(
    x,
    breaks = c(-Inf, -2, 0, 2, Inf),
    labels = c("dark red (< -2)", "pink (-2 to 0)", "light blue (0 to 2)", "dark blue (> 2)")
  )
}

shps_sf$coefPCTVACANT_cat <- categorize_ratio(shps_sf$coefPCTVACANT_st)
shps_sf$coefPCTSINGLES_cat <- categorize_ratio(shps_sf$coefPCTSINGLES_st)
shps_sf$coefPCTBACHMOR_cat <- categorize_ratio(shps_sf$coefPCTBACHMOR_st)
shps_sf$coefLNNBELPOV_cat <- categorize_ratio(shps_sf$coefLNNBELPOV_st)

# --- 5. Define manual colors ---
ratio_colors <- c(
  "dark red (< -2)" = "darkred",
  "pink (-2 to 0)" = "pink",
  "light blue (0 to 2)" = "lightblue",
  "dark blue (> 2)" = "darkblue"
)

# --- 6. Plot all predictors (example for PCTVACANT) ---
ggplot(shps_sf) +
  geom_sf(aes(fill = coefPCTVACANT_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (PCTVACANT)") +
  labs(title = "Standardized GWR Coefficients for PCTVACANT") +
  theme_minimal() +
  theme(legend.position = "right")

ggplot(shps_sf) +
  geom_sf(aes(fill = coefPCTSINGLES_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (PCTSINGLES)") +
  labs(title = "Standardized GWR Coefficients for PCTSINGLES") +
  theme_minimal() +
  theme(legend.position = "right")

ggplot(shps_sf) +
  geom_sf(aes(fill = coefPCTBACHMOR_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (PCTBACHMOR)") +
  labs(title = "Standardized GWR Coefficients for PCTBACHMOR") +
  theme_minimal() +
  theme(legend.position = "right")

ggplot(shps_sf) +
  geom_sf(aes(fill = coefLNNBELPOV_cat), color = NA) +
  scale_fill_manual(values = ratio_colors, name = "Coef/SE (LNNBELPOV)") +
  labs(title = "Standardized GWR Coefficients for LNNBELPOV") +
  theme_minimal() +
  theme(legend.position = "right")

```
The spatial patterns in the local coefficients demonstrate clear heterogeneity across Philadelphia.
For vacancy rate (PCTVACANT), the strongest negative effects on housing values are found in North and Southwest Philadelphia, indicating that higher vacancy rates are particularly detrimental to property values in areas with already weak housing markets.
In contrast, the proportion of single-family homes (PCTSINGLES) shows a mixed pattern: positive effects in the northeastern and northwestern suburbs, where detached homes are more desirable, but weak or negative associations in older central neighborhoods dominated by rowhouses.
The coefficient for educational attainment (PCTBACHMOR) is positive across nearly all block groups, with the strongest effects concentrated in University City, Center City, and Northwest Philadelphia, confirming the robust link between human capital and housing value.
Finally, poverty (LNNBELPOV) exhibits a negative effect in North and South Philadelphia, showing the spatial concentration of disadvantage and its inverse relationship with property values.
Together, these patterns highlight how the relationships between neighborhood characteristics and housing values are spatially nonstationary, emphasizing the importance of local context in housing market analysis.

# Discussion (Google Doc at end)