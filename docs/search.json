[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Note: These reports were originally prepared as PDFs rather than HTML pages. Each page includes a link to view the PDF version, which may be preferable if you encounter any rendering issues in the browser.\n\n\n\nIn this analysis, we use a multiple linear regression model to predict median house values in Philadelphia. Drawing on Philadelphia’s tract-level census data, we examine the impact of our four predictors on our response variable block group median house value: percentage with at least a bachelor’s degree, percentage of vacant spaces, number living below the poverty line, and percentage of single family housing units.\n\n\n\n\n\nHere, we use a set of spatial models to better account for geographic dependence in housing values across Philadelphia. Because housing markets and neighborhood conditions are not independent across space, we apply spatial lag and spatial error models to capture spillover effects between nearby block groups. We also use geographically weighted regression to allow the relationships between housing values and our predictors to vary across the city, helping us understand how these effects differ by location rather than assuming a single citywide relationship.\n\n\n\n\n\nThis analysis examines alcohol-impaired driving crashes in Philadelphia and the factors associated with whether a crash involved a drinking driver. Using crash-level data linked to neighborhood demographics, we look at how crash characteristics, age groups, and surrounding socioeconomic conditions relate to alcohol involvement. We use a logistic regression model to estimate the likelihood that a given crash was alcohol-related and to explore how these relationships vary across the city.\n\n\n\n\n\nIn this analysis, we performed text-mining techniques to movie reviews from the Internet Movie Database (IMDb) in order to quantify and visualize word trends and emotional tones across reviews."
  },
  {
    "objectID": "index.html#navigate-the-project-website",
    "href": "index.html#navigate-the-project-website",
    "title": "Welcome",
    "section": "Navigate The Project Website",
    "text": "Navigate The Project Website\n\nUsing OLS Regression to Predict Median House Values in Philadelphia\nThe project report detailing methodology, results, and conclusions. This includes generated images and html.\n\n\nVisual Components\nLook here if you are only interested in generated figures and html."
  },
  {
    "objectID": "HW_1/Homework1.html",
    "href": "HW_1/Homework1.html",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "",
    "text": "In this analysis, we use a multiple linear regression model to predict median house values in Philadelphia. Drawing on Philadelphia’s tract-level census data, we examine the impact of our four predictors on our response variable block group median house value: percentage with at least a bachelor’s degree, percentage of vacant spaces, number living below the poverty line, and percentage of single family housing units.\nPrior theoretical knowledge of the relationships between housing markets and socioeconomic factors has led us to hypothesize a relationship between these four predictors and median house value. High rates of educational attainment and single-family homes are likely positively associated with house values as they may indicate neighborhood stability by signaling higher earning potential and long-term residency. Conversely, high rates of vacancy and poverty levels are likely negatively associated with house values as they may indicate neighborhood instability through a lack of high earning residents and occupants overall. In this analysis, we aim to assess the explanatory power of these predictors and briefly explore if these relationships possess any spatial patterns."
  },
  {
    "objectID": "HW_1/Homework1.html#data-cleaning",
    "href": "HW_1/Homework1.html#data-cleaning",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "2.1 Data Cleaning",
    "text": "2.1 Data Cleaning\nThe original dataset contained 1,816 Census block groups across Philadelphia. Following the data preparation protocol provided in the assignment, we removed four types of block groups:\n1. Those with fewer than 40 residents,\n2. Those with no housing units,\n3. Those with median house values below $10,000, and\n4. One extreme outlier in North Philadelphia with an exceptionally high median house value (over $800,000) and a very low median household income (below $8,000).\nAfter applying these filters, the final cleaned dataset consisted of 1,720 block groups. All variables used in this analysis were numeric and contained no missing values after cleaning.\nThe cleaned dataset includes the following key variables used in subsequent modeling:\n\nMEDHVAL – Median house value (dependent variable)\n\nPCTBACHMOR – Percentage of residents with at least a bachelor’s degree\n\nPCTVACANT – Percentage of housing units that are vacant\n\nPCTSINGLES – Percentage of detached single-family homes\n\nNBELPOV100 – Number of households below the poverty line\n\nAll variables were obtained from the American Community Survey (ACS) 5-year estimates at the block group level for Philadelphia. The variables were selected to capture major socioeconomic and housing characteristics relevant to neighborhood housing values."
  },
  {
    "objectID": "HW_1/Homework1.html#exploratory-data-analysis",
    "href": "HW_1/Homework1.html#exploratory-data-analysis",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "2.2 Exploratory Data Analysis",
    "text": "2.2 Exploratory Data Analysis\n\n2.2.1 Variable Distributions\nExploratory Data Analysis (EDA) was conducted to assess variable distributions, detect potential skewness, and identify relationships among predictors prior to model estimation. The objective of this step was to ensure that the data met the assumptions required for Ordinary Least Squares (OLS) regression and to guide appropriate variable transformations.\nFirst, frequency histograms and scatterplots were used to visually assess the distributional properties of both the dependent variable and the explanatory variables. These graphical tools help determine whether linearity and normality assumptions are reasonably satisfied.\nWhen strong right-skewness or heteroskedasticity was observed, logarithmic transformations were applied to stabilize variance and improve normality. The general transformation function used is expressed as: \\[\ny_i' = \\ln(1 + y_i)\n\\] where \\(y_i\\) represents the original variable and \\(y_i'\\) is the transformed variable.\nTo examine relationships among predictors and detect potential multicollinearity, the Pearson correlation coefficient was computed for each pair of independent variables, using the formula: \\[\nr = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}\n         {\\sqrt{\\sum (x_i - \\bar{x})^2}\\sqrt{\\sum (y_i - \\bar{y})^2}}\n\\] where \\(\\bar{x}\\) is the sample mean of the predictor, and \\(\\bar{y}\\) is the sample mean of the dependent variable.\nVisualization techniques, including choropleth mapping using sf and ggplot2, were employed to explore potential spatial clustering or spatial dependence among variables. These visual checks provide preliminary evidence for whether OLS assumptions of spatial independence are likely to be met."
  },
  {
    "objectID": "HW_1/Homework1.html#multiple-regression-analysis",
    "href": "HW_1/Homework1.html#multiple-regression-analysis",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "2.3 Multiple Regression Analysis",
    "text": "2.3 Multiple Regression Analysis\nMultiple regression models a dependent variable as a function of multiple predictors, rather than a single predictor such as in simple regression. These predictors each have a coefficient that represents their effect on a dependent variable, controlling for all other predictors. This approach improves model accuracy in situations where multiple variables better explain outcomes of a dependent variable.\nThis report regressed log-transformed median house value (LNMEDHVAL) on the proportion of housing units that are vacant (PCTVACANT), percent of housing units that are single family detached homes (PCTSINGLES), proportion of residents with at least a bachelor’s degree (PCTBACHMOR), and log-transformed number of households with incomes below 100% poverty level (LNNBELPOV). This regression function can be expressed as follows: \\[\n\\text{LNMEDHVAL} = \\beta_0 + \\beta_1 \\text{PCTVACANT} + \\beta_2 \\text{PCTSINGLES} + \\beta_3 \\text{PCTBACHMOR} + \\beta_4 \\text{LNNBELPOV} + \\varepsilon\n\\] Multiple regression relies on several key assumptions, most of which mirror the assumptions of simple regression. First, linear relationships should exist between the dependent variable and each predictor, assessed through scatterplots or residual plots and addressed via transformations if needed. Second, residuals should be approximately normally distributed, which can be assessed through a histogram. Third, residuals must be random — indicating that observations are not systematically related. Fourth, residuals must be homoscedastic, exhibiting constant variance across all values. Fifth, the dependent variable should be continuous.\nA unique assumption for multiple regression is avoiding perfect multicollinearity: no predictor should be strongly correlated with others. Multicollinearity inflates standard errors and produces unstable coefficient estimates. This assumption can be checked by analyzing the correlation coefficients between all dependent variables, with anything greater than 0.9 generally being a cause for concern. Variance Inflation Factor (VIF) can be used to further inspect a suspicion of multicollinearity, with a VIF &lt; 5 being generally acceptable and a VIF &lt; 10 warranting more inspection. A VIF &gt; 10 strongly indicates multicollineariy.\nIn the above multiple regression function, \\(\\beta_0\\) represents the depedent variable when all predictors are zero. The coefficients of the predictors \\(\\beta_1, \\beta_2, \\beta_3, \\beta_4\\) each represent the change in the dependent variable with a one unit increase in the predictor, holding all other predictors constant.\nThese \\(\\beta\\) coefficients in multiple regression are simultaneously estimated in order to minimize the Error Sum of Squares (SSE). The general formula and breakdown of what is to be minimized is provided below (with n being the number of observations, and k is the number of predictors): \\[\nSSE = \\sum_{i=1}^{n} \\varepsilon^2\n     = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n     = \\sum_{i=1}^{n} \\left[ y_i - \\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1i} + \\hat{\\beta}_2 x_{2i} + \\cdots + \\hat{\\beta}_k x_{ki} \\right) \\right]^2\n\\]\nThis minimization works by finding the \\(\\beta\\) coefficients that, when raw predictor \\((x_{i})\\) data is used, will minimize the residuals \\((y_i - \\hat{y}_i)\\). SSE is also used to calculate Mean Squared Error (MSE), noted by the estimated parameter \\(\\hat\\sigma^2\\). This is the estimate of the variance of the error term \\(\\epsilon\\). The formula for MSE, in terms of SSE is noted below: \\[\nMSE = \\frac{SSE}{n - (k+1)}\n\\]\nAnother term in regression analysis is Total Sum of Squares (SST). It measures the total variation in the dependent variable around it’s mean by using the following formula: \\[\nSST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n\\] Using this formula for SST, and the previously stated formula for SSE, we can calculate \\(R^2\\) — the coefficient of multiple determination. This is the proportion of variance in the model explained by all k predictors, and is the represented by the following: \\[\nR^2 = 1 - \\frac{\\text{SSE}}{\\text{SST}}\n\\] Multiple regression presents a unique dillema in comparison to simple regression, in that adding more predictors will generally increase \\(R^2\\). Adjusting \\(R^2\\), noted below, can account for additional predictors and determine whether or not they are improving the model. \\[\nR_{\\text{adj}}^2 = \\frac{(n-1) R^2 - k}{n - (k+1)}\n\\] This report will conduct two tests to evaluate the model. First, there is the F-ratio — a model utility test. F-ratio tests the following null hypothesis \\(H_0\\) and alternative hypothesis \\(H_a\\): \\[\nH_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\n\\] \\[\nH_a: \\text{At least one } \\beta_i \\neq 0\n\\] In essence, the null hypothesis states that all of the model \\(\\beta\\) parameters (except \\(\\beta_0\\), which is not a predictor coefficient) are zero, and the alternative states that at least one of those parameters is not zero. Failure to reject the null hypothesis suggests that the model is incredibly weak, and should be reevaluated. If the null hypothesis is rejected, the second test can be conducted with the following hypotheses. \\[\nH_0: \\beta_i = 0\n\\] \\[\nH_a: \\beta_i \\neq 0\n\\] In this test, we evaluate the performance of each predictor i (in the case of this report, the 4 predictors stated earlier). A t-test can be conducted, where the t-statistic for each predictor is calculated as the estimated coefficient divided by its standard error: \\[\nt_i = \\frac{\\hat{\\beta}_i - \\beta_i}{s_{\\hat{\\beta}_i}}\n\\] Each predictor has its own p-value calculated using the above t-statistic. If the p-value is less than 0.05, we reject the null hypothesis for that predictor and conclude that it is a statistically significant predictor of the dependent variable. If the p-value is greater than or equal to 0.05, we fail to reject the null hypothesis and conclude that the predictor is not statistically significant."
  },
  {
    "objectID": "HW_1/Homework1.html#additional-analysis",
    "href": "HW_1/Homework1.html#additional-analysis",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "2.4 Additional Analysis",
    "text": "2.4 Additional Analysis\nUsing the stepAIC() and step$anova command, we applied bidirectional stepwise regression to analyze the fit of our linear model. Stepwise regression determines the minimum number of predictors that yield the best model. Stepwise regression automatically selects or eliminates predictors, either forwards, backwards, or bidirectionally, based on some type of criteria that measures the goodness of fit. In this case, we are attempting to determine the predictor or combination of predictors that minimize the Akaike Information Criterion (AIC). AIC is an estimator of predictor error and provides insight into the quality of the model by penalizing increasing number of predictors that could lead to over-fitting.\nStepwise regression, however, poses many limitations as it does not consider theoretical relevance of the predictors, may overlook alternative valid models, and runs the risk of excluding important predictors and including unimportant predictors, especially due to the numerous t-tests measuring whether the null hypothesis, \\(\\beta_k\\) = 0, is true.\nTo perform cross-validation, we used the trainControl() function with the method parameter set to “cv” (cross-validation) and the train() function with the method parameter set to “lm” (linear model). Cross-validation is a technique that measures model performance unbiasedly by training the model on a select subgroup of observations and seeing how well it estimates deliberately excluded observations. K-fold cross validation where k=5, specifically, divides data sets into five non-overlapping folds and repeatedly uses four folds for training the model and one fold for validating the model so that each fold trains the model multiple times and validates the model once. This method ensures a model’s generalizability to new data and minimizes distortion by avoiding omitting and duplicating data in its measure of fit. The Root Mean Square Error (RMSE) is the summary of the model’s performance across all folds. For each fold, the average squared difference or Mean Squared Error (MSE) is calculated as the average squared difference between predicted values, \\(hat{y}_i\\), estimated by the model’s \\(\\beta\\) coefficient, and the actual value, \\(y_i\\).The RMSE is then calculated by taking the square root of the average MSE of all five folds. The complete formula for RMSE is as follows: \\[\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\] After performing k-fold cross-validation on two or more models, the RMSE of the models can be compared to determine which model has the best performance. A smaller RMSE indicates that the model’s predictions are, on average, closer to the actual values, and thus more representative of the data."
  },
  {
    "objectID": "HW_1/Homework1.html#software-used",
    "href": "HW_1/Homework1.html#software-used",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "2.5 Software Used",
    "text": "2.5 Software Used\nAll data analysis was conducted using R. Within R, the following packages were used to perform data preparation, exploratory analysis, regression modeling, and visualization: ggplot, dplyr, sf, patchwork, MASS, and caret."
  },
  {
    "objectID": "HW_1/Homework1.html#exploratory-results",
    "href": "HW_1/Homework1.html#exploratory-results",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "3.1 Exploratory Results",
    "text": "3.1 Exploratory Results\nDistribution of Variables The distributions of the dependent variable (MEDHVAL) and four key predictors were first examined using histograms. All variables were positively skewed, particularly median house value and poverty rate.\n\n\n\n\n\n\n\n\n\nTo explore if transformations were needed to adjust the distributions of these predictors, we log transformed them and generated the following histograms:\n\n\n\n\n\n\n\n\n\nThe above histograms show that transforming PCTBACHMOR, PCTVACANT, and PCTSINGLES results in distributions that are zero-inflated — that is, they have a huge spike at zero. This suggests that a log-transformation is not appropriate for these predictors. The dependent variable MEDHVAL, and the predictor NBELPOV100 are normally distributed after the log transformation, so this appropriate for these variables.\nSummary Statistics Table Table 1 summarizes the mean and standard deviation of all key variables.\n\n\n**Table 1. Summary statistics for dependent and predictor variables**\n\n\n\n\n\nVariable\nMean\nSD\n\n\n\n\nMedian House Value (MEDHVAL)\n66287.73\n60006.08\n\n\nHouseholds Below Poverty (NBELPOV100)\n189.77\n164.32\n\n\n% with Bachelor’s or Higher (PCTBACHMOR)\n16.08\n17.70\n\n\n% Detached Single-Family Homes (PCTSINGLES)\n9.23\n13.25\n\n\n% Vacant Housing Units (PCTVACANT)\n11.29\n9.63\n\n\n\n\n\nLog Transformations\nBecause the variables were positively skewed, log transformations were applied to stabilize variance and improve normality.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Analysis To check for multicollinearity, Pearson correlation coefficients were computed among the predictors.\n\n\n             PCTBACHMOR LNNBELPOV100  PCTVACANT PCTSINGLES\nPCTBACHMOR    1.0000000   -0.3197668 -0.2983580  0.1975461\nLNNBELPOV100 -0.3197668    1.0000000  0.2495470 -0.2905159\nPCTVACANT    -0.2983580    0.2495470  1.0000000 -0.1513734\nPCTSINGLES    0.1975461   -0.2905159 -0.1513734  1.0000000\n\n\nCorrelations were moderate: the strongest being between PCTBACHMOR and LNNBELPOV100 (r = –0.32). This indicates that the predictors were not highly collinear and all could be retained in the regression model.\nSpatial Patterns Finally, choropleth maps were created to visualize the spatial patterns of median house value and predictor variables across Philadelphia."
  },
  {
    "objectID": "HW_1/Homework1.html#regression-results",
    "href": "HW_1/Homework1.html#regression-results",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "3.2 Regression Results",
    "text": "3.2 Regression Results\nThe output of the regression model (\\(\\text{LNMEDHVAL} = \\beta_0 + \\beta_1 \\text{PCTVACANT} + \\beta_2 \\text{PCTSINGLES} + \\beta_3 \\text{PCTBACHMOR} + \\beta_4 \\text{LNNBELPOV} + \\varepsilon\\)) in R is as follows.\n\n\n\nCall:\nlm(formula = LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + \n    LNNBELPOV, data = Regression_shpData)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.25817 -0.20391  0.03822  0.21743  2.24345 \n\nCoefficients:\n              Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept) 11.1137781  0.0465318 238.843 &lt; 0.0000000000000002 ***\nPCTVACANT   -0.0191563  0.0009779 -19.590 &lt; 0.0000000000000002 ***\nPCTSINGLES   0.0029770  0.0007032   4.234            0.0000242 ***\nPCTBACHMOR   0.0209095  0.0005432  38.494 &lt; 0.0000000000000002 ***\nLNNBELPOV   -0.0789035  0.0084567  -9.330 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3665 on 1715 degrees of freedom\nMultiple R-squared:  0.6623,    Adjusted R-squared:  0.6615 \nF-statistic: 840.9 on 4 and 1715 DF,  p-value: &lt; 0.00000000000000022\n\n\nWe regressed the natural log of median house value (LNMEDHVAL) on the percentage of vacant houses (PCTVACANT), percentage of single-family houses (PCTSINGLES), percentage of residents with a bachelor’s degree or higher (PCTBACHMOR), and the natural log of the neighborhood poverty rate (LNNBELPOV). All four predictors are statistically significant with p-values far below a threshold of p &lt; 0.05.\nThe log-transformation of median house value (LNMEDHVAL, the dependent variable) means that we can interpret the coefficients as percent changes in median home value for a one unit change in the predictor. A one percentage point increase in vacant houses (PCTVACANT) is associated with an approximate 1.92% decrease in median home value. A one percentage point increase in single-family houses (PCTSINGLES) is associated with an approximate 0.30% increase in median home value. A one percentage point increase in residents with a bachelor’s degree or higher (PCTBACHMOR) is associated with a roughly 2.09% increase in median home value. For LNNBELPOV — a log-transformed predictor — a 1% increase in the number of people in poverty corresponds to an approximate 0.079% decrease in median home value.\nThe very low p-values indicate that if there were actually no relationship between each predictor and median home value (i.e., \\(H_0: \\beta_i = 0\\)), the probability of observing the estimated coefficients we see would be very close to zero. Therefore, we can reject the null hypotheses for all predictors \\(H_0: \\beta_i = 0\\).\nThe model explains a substantial portion of the variance in median home values, with \\(R^2 = 0.6623\\) and \\(R^2_{adj} = 0.6615\\). The F-statistic is highly significant, with \\(F = 840.9 \\text{ and a p-value of } p &lt; 0.00000000000000022\\), allowing us to reject the \\(H_0\\) that all coefficients in the model are 0."
  },
  {
    "objectID": "HW_1/Homework1.html#regression-assumption-checks",
    "href": "HW_1/Homework1.html#regression-assumption-checks",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "3.3 Regression Assumption Checks",
    "text": "3.3 Regression Assumption Checks\nIn this section, we assess whether our multiple regression model meets key assumptions and take the necessary steps to address any violations of these assumptions. Early visualizations of the distribution of the predictors PCTBACHMOR, NBELOWPOV100, PCTVACANT, and PCTSINGLES and the dependent variable MEDHVAL were presented by histograms which all showed positively-skewed distributions for all predictors. While multiple regression assumptions require the normality of residuals and not predictor values, non-normal distribution of predictors values can indicate violations of the assumptions of non-normal residuals and a lack of linearity.\n\n\n\n\n\n\n\n\n\nThe skewedness of histograms of each predictor is reflected in the scatter plots of the predictors by the dependent variable median house value, MEDHVAL, as, as the predictor values increase, y values cluster towards lower values. The lack of linearity between the predictor and MEDHVAL and the skewed distribution of predictor values suggest that some type of nonlinear transformation may need to occur in order to normally distribute values and achieve linearity. We performed log transformations which are commonly used to correct the positively skewed distributions evident in our variables.\n\n\n\n\n\n\n\n\n\nWe applied logarithmic transformations to all predictors and the dependent variable to see whether the transformations would improve distribution of their values and subsequently allow us to assume linearity and residual normality. We only substituted the log-transformed MEDHVAL (LNMEDHVAL) and log-transformed NBELOPOV (LNNBELOPOV) for the rest of our analysis. The log-transformed predictors PCTVACANT, PCTSINGLES, and PCTBACHMOR did not return an improvement and instead produced zero-inflated distributions. We proceeded to calculate our standardized residuals with the new model of original predictors PCTVACANT, PCTSINGLES, and PCTBACHMOR and with our log-transformed predictor LNNBELPOV by our log transformed dependent variable LNMEDHVAL. The histogram of the standardized residuals show the normality in residuals needed per our assumption and support the need for the logarithmic transformations.\n\n\n\n\n\n\n\n\n\nStandardized residuals are residuals divided by their standard deviation as a means to prime residuals across different observations for comparison through normalization. The scatter plot of our standardized residuals shows general homoscedasticity or consistent variance of residuals. There is general uniformity of the standardized residuals as most lie between -2 and positive 2. There are some outliers that extend past -4 and 4 but they do not dominate the overall pattern. There is also no funneling affect or any other pattern of non-constant variance. Thus, our model satisfies the assumption of homoscedasticity of residuals.\nInitial spatial visualizations of the dependent and predictor variables suggest that there may be some spatial autocorrelation between their respective measurements. The choropleth map of the logged dependent variable LNMEDHVAL shows that lower values seem to be concentrated in parts of North, Southwest, and West Philadelphia while higher values were clustered in Upper North Philadelphia. The choropleth map of the predictor PCTSINGLES shows higher percentages in parts of Upper North and Northeast Philadelphia. The choropleth map of the predictor PCTBACHMOR shows higher percentages in parts of Upper North and Center Philadelphia. The choropleth map of the logged predictor LNNBELPOV showed lower values in parts of Upper North, Northeast, and Center Philadelphia. The choropleth map of the predictor PCTVACANT shows higher percentages in parts of North, West, Southwest, South, and Center Philadelphia.This visual inspection suggests that block groups might not be entirely independent of each other and could require further spatial assessment.\n\n\n\n\n\n\n\n\n\nThe choropleth of standardized residuals suggests possible spatial autocorrelation as there seems to be a concentration of lower values in the southern half of Philadelphia. Visually, there seems to be a gradient effect stemming outward from North Philadelphia into West, Southwest, and South Philadelphia. This indicates that their could be additional factors producing systematic under prediction in the southern half of Philadelphia, especially in North Philadelphia."
  },
  {
    "objectID": "HW_1/Homework1.html#additional-models",
    "href": "HW_1/Homework1.html#additional-models",
    "title": "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia",
    "section": "3.4 Additional Models",
    "text": "3.4 Additional Models\nStepwise Regression ANOVA table\n\n\nStart:  AIC=-3448.16\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV\n\n             Df Sum of Sq    RSS     AIC\n&lt;none&gt;                    230.33 -3448.2\n- PCTSINGLES  1     2.407 232.74 -3432.3\n- LNNBELPOV   1    11.692 242.02 -3365.0\n- PCTVACANT   1    51.543 281.87 -3102.8\n- PCTBACHMOR  1   199.014 429.35 -2379.0\n\n\nStepwise Model Path \nAnalysis of Deviance Table\n\nInitial Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV\n\nFinal Model:\nLNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV\n\n\n  Step Df Deviance Resid. Df Resid. Dev       AIC\n1                       1715   230.3317 -3448.162\n\n\nOur initial model before performing stepwise regression: \\[\n\\text{LNMEDHVAL} \\sim \\text{PCTVACANT} + \\text{PCTSINGLES} + \\text{PCTBACHMOR} + \\text{LNNBELPOV}\n\\] As mentioned earlier, stepwise regression based on AIC evaluates whether a predictor improves the model fit by reducing the AIC. Our initial model had an AIC of -3448.162. When PCTSINGLES was removed, the AIC increased to –3432.3. When LNNBELPOV was removed, the AIC increased to –3365.0. When PCTVACANT was removed, the AIC increased to –3102.8. When our last predictor PCTBACHMOR was removed, the AIC increased drastically to –2379.0. Since the removal of each predictor resulted in a higher AIC, all four initial predictors were retained in the final model. This suggests that the initial model was selected by stepwise regression as being a model that balances explanatory power and complexity.\nK-fold Cross-validation Table\n\n\nLinear Regression \n\n1720 samples\n   5 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1376, 1376, 1376, 1376, 1376 \nResampling results:\n\n  RMSE      Rsquared   MAE      \n  0.367231  0.6621381  0.2723487\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nLinear Regression \n\n1720 samples\n   3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 1376, 1376, 1376, 1376, 1376 \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.4429013  0.5098874  0.3179965\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nWe performed 5 fold cross-validation on two models, the first model including all of our original predictors and the second model being a reduced set of predictors that alternatively included MEDHHINC as a predictor. The second model is as follows:\n\\[\n\\text{LNMEDHVAL} \\sim \\text{PCTVACANT} + \\text{MEDHHINC}\n\\]\nThe original model yielded a RMSE of 0.368 while the reduced model yielded a RMSE of 0.443, signaling that the additional predictors in the full model had better predictive power compared to PCTVACANT and MEDHHINC alone."
  },
  {
    "objectID": "HW_3/Homework3.html",
    "href": "HW_3/Homework3.html",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "",
    "text": "Car crashes involving alcohol-impaired driving are a major public safety issue in the United States, contributing to almost 30 deaths every day. Understanding the factors that make an alcohol-related crash more or less likely is critical towards deploying effective interventions. This report focuses on 43,364 crashes that occurred in Philadelphia’s residential block groups, where demographic and socioeconomic data is available, to study the association between alcohol-impaired driving crashes and certain predictors.\nThe predictors included in this study reflect both crash conditions and the characteristics of the surrounding neighborhood. Predictors such as speeding, aggressive driving, or a fatal or major-injury outcome were explored because of potential links to risky behavior that occurs with alcohol use, while age-related variables capture groups that are statistically over- or under-represented in drunk-driving crashes. Neighborhood indicators like median household income and educational attainment may also be associated with spatial patterns of alcohol-involved crashes. To examine these relationships, we use R to run a logistic regression model that predicts the probability that a given crash involved a drinking driver."
  },
  {
    "objectID": "HW_3/Homework3.html#logistic-regression-motivation-and-foundation",
    "href": "HW_3/Homework3.html#logistic-regression-motivation-and-foundation",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "2.1 Logistic Regression Motivation and Foundation",
    "text": "2.1 Logistic Regression Motivation and Foundation\nThe dependent variable in this report (DRINKING_D) is a binary indicator: it takes the value 1 (True) or 0 (False). This presents a major limitation for Ordinary Least Squares (OLS) regression. OLS assumes a continuous dependent variable and estimates coefficients that can take any value from \\(-\\infty\\) to \\(+\\infty\\), interpreting each coefficient as the expected change in the dependent variable for a one-unit increase in a predictor. When the outcome is binary, this interpretation breaks down. A predicted value of 0.65, for example, is not meaningful when the outcome can only be 0 or 1.\nBecause of these issues, logistic regression is more appropriate for modeling a binary outcome. Logistic regression works by transforming the probability of the event into a metric that can take any real value: the log-odds (also called the logit). To understand this transformation, it helps to introduce the concept of odds.\nWhile probability is defined as \\(\\Pr(\\text{event}) = \\frac{\\#\\text{desirable outcomes}}{\\#\\text{possible outcomes}}\\), the odds of an event are defined as \\(\\frac{\\#\\text{desirable outcomes}}{\\#\\text{undesirable outcomes}}\\). In the context of this report, the odds of drink-driving are \\(\\frac{\\#\\text{with drink driving}}{\\#\\text{without drink driving}}\\).\nLogistic regression models the log of the odds, or the logit, as a linear function of the predictors. Because the log-odds range from \\(-\\infty\\) to \\(+\\infty\\), the model avoids the limitations of OLS. Exponentiating a logistic regression coefficient produces an odds ratio (OR), which describes how the odds of the outcome change for a one-unit increase in a predictor. An \\(OR &gt; 1\\) indicates increased odds of the event, while an \\(OR &lt; 1\\) indicates decreased odds.\nTo model the probability that a crash involved a drinking driver, we use a logistic regression model with DRINKING_D as the dependent variable and a set of binary and continuous predictors. The binary predictors indicate whether specific conditions applied to the crash: whether the crash resulted in a fatality or major injury (FATAL_OR_M), whether the vehicle was overturned (OVERTURNED), whether the driver was using a cell phone at the time of the crash (CELL_PHONE), whether the crash involved speeding (SPEEDING), whether aggressive driving was involved (AGGRESSIVE), whether at least one driver was 16 or 17 years old (DRIVER1617), or whether at least one driver was 65 or older (DRIVER65PLUS). In addition, the model includes continuous block-group-level predictors, specifically the percent of adults with at least a bachelor’s degree (PCTBACHMOR) and the median household income (MEDHHINC) for the location where the crash occurred. For this report, the logit model expresses the log-odds of a drinking-driver crash as a linear function of the predictors. The model is:\n\\[\n\\begin{aligned}\n\\ln\\left(\\frac{p}{1 - p}\\right) =\\;&\n\\beta_0\n+ \\beta_1 \\text{(FATAL OR M)}\n+ \\beta_2 \\text{(OVERTURNED)}\n+ \\beta_3 \\text{(CELL PHONE)} \\\\\n&+ \\beta_4 \\text{(SPEEDING)}\n+ \\beta_5 \\text{(AGGRESSIVE)}\n+ \\beta_6 \\text{(DRIVER1617)} \\\\\n&+ \\beta_7 \\text{(DRIVER65PLUS)}\n+ \\beta_8 \\text{(PCTBACHMOR)}\n+ \\beta_9 \\text{(MEDHHINC)}.\n\\end{aligned}\n\\]\nHere, \\(p\\) is the probability that \\(\\text{DRINKING\\_D} = 1\\), meaning the crash involved a drinking driver. The term \\(\\ln\\left(\\frac{p}{1-p}\\right)\\) is the logit, or the natural log of the odds of a drinking-driver crash. Each \\(\\beta_k\\) represents the change in the log-odds associated with a one-unit increase in the corresponding predictor, holding the others constant.\nBinary predictors such as \\(\\text{FATAL\\_OR\\_M}\\) or \\(\\text{SPEEDING}\\) shift the log-odds by \\(\\beta_k\\) when the indicator changes from 0 to 1. Continuous predictors such as \\(\\text{PCTBACHMOR}\\) and \\(\\text{MEDHHINC}\\) shift the log-odds proportionally to their values.\nWe can rewrite the model by solving for \\(p = P(\\text{DRINKING\\_D} = 1)\\) (note: to make the formula fit on the report, we used z to notate the \\(\\beta\\) coefficients and predictors) :\n\\[\n\\begin{aligned}\np &= \\frac{1}{1 + e^{-z}} = \\frac{e^z}{1 + e^z} \\\\[10pt]\n\\text{where } z =\\;& \\beta_0 + \\beta_1 \\text{(FATAL OR M)} + \\beta_2 \\text{(OVERTURNED)} + \\beta_3 \\text{(CELL PHONE)} \\\\\n&+ \\beta_4 \\text{(SPEEDING)} + \\beta_5 \\text{(AGGRESSIVE)} + \\beta_6 \\text{(DRIVER1617)} \\\\\n&+ \\beta_7 \\text{(DRIVER65PLUS)} + \\beta_8 \\text{(PCTBACHMOR)} + \\beta_9 \\text{(MEDHHINC)}\n\\end{aligned}\n\\]\nThis expression uses the logistic function, which transforms any real-valued input into a valid probability between \\(0\\) and \\(1\\). The denominator, with the exponentiation, ensures that \\(p\\) is always bounded between \\(0\\) and \\(1\\), regardless of the values of the predictors or coefficients."
  },
  {
    "objectID": "HW_3/Homework3.html#hypothesis-testing-overview",
    "href": "HW_3/Homework3.html#hypothesis-testing-overview",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "2.2 Hypothesis Testing Overview",
    "text": "2.2 Hypothesis Testing Overview\nIn logistic regression, each predictor \\(x_i\\) is tested for the null hypothesis, \\(H_0\\), that the beta coefficient, \\(\\beta_i\\), is 0 against the alternative hypothesis \\(H_a\\) that \\(\\beta_i\\) is not 0: \\[\nH_0: \\beta_i = 0\n\\] \\[\nH_a:  \\beta_i \\neq 0\n\\] The z-value, also known as the Wald statistic in logistic regression, is the test statistic that we calculate under the null hypothesis. We calculate this statistic by dividing the estimated beta coefficient, \\(\\hat{\\beta}_i\\), by its standard error or \\(\\sigma_{\\hat{\\beta}_i}\\):\n\\[\nz = \\frac{\\hat{\\beta}_i}{\\sigma_{\\hat{\\beta}_i}}\n\\]\nUnder the null hypothesis, the Wald statistic follows an approximately standard normal distribution, N(0,1). This property allows us to compute the two‑tailed p‑value as the probability of observing a statistic as extreme, or more extreme, than the calculated statistic if the null hypothesis were true. If the p-value is &lt; 0.05, we can reject the null hypothesis in favor of the alternative hypothesis that \\(\\beta_i\\) is not 0. Rather than interpreting the raw beta coefficients, statisticians prefer use the odds ratio, \\(OR_i\\), which can be calculated by exponentiating \\(\\hat{\\beta}_i\\):\n\\[\nOR_i = e^{\\hat{\\beta}_i}\n\\]\nThe odds ratio expresses the effect of a predictor on the dependent variable in multiplicative terms. Specifically, it represents how the odds of the event change for a one‑unit increase in the predictor, holding other variables constant. The null and alternative hypothesis can be adapted for the odds ratio, where the null hypothesis is the predictor has no effect on the odds (\\(OR = 1\\)) and the alternative hypothesis is that the predictor increases or decrease the odds of the event (\\(OR \\neq 1\\)): \\[\nH_0: OR = 1\n\\] \\[\nH_a: OR \\neq 1\n\\] Conceptually, the odds ratio is the ratio of the odds with the predictor present to the odds with the predictor absent. Thus, if the odds ratio equals 1, it indicates that the odds are the same: the predictor did not change the odds of the outcome. Alternatively, if the odds ratio is significantly above or below 1, the predictor increased or decreased the odds. The confidence intervals for the odds ratios can be calculated by exponentiating the coefficient confidence intervals. These intervals provide a range of plausible values for the true odds ratio, reflecting the uncertainty of the estimate. In the context of logistic regression, the presence of a 1 in the confidence interval indicates the predictor’s effect is not statistically significant while a confidence interval entirely above or below 1, indicates that the predictor increased or decreased the odds.\nAll coefficient estimates, z‑values, and p‑values were extracted in R from the fitted logistic regression model’s summary. Odds ratios and their confidence intervals were calculated by exponentiating the original coefficient estimates and confidence intervals, then merged with the extracted coefficients for interpretation."
  },
  {
    "objectID": "HW_3/Homework3.html#assessing-model-quality-of-fit",
    "href": "HW_3/Homework3.html#assessing-model-quality-of-fit",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "2.3 Assessing Model Quality of Fit",
    "text": "2.3 Assessing Model Quality of Fit\nIn our analysis, goodness of the model’s fit was evaluated in various ways. In Ordinary Least Squares (OLS) regression, \\(R^2\\) is used to evaluate model fit as it is a statistic that returns the proportion of total variance in the dependent variable explained by the independent variable. Unlike in OLS regression, logistic regression doesn’t model a continuous outcome. In logistic regression the dependent variable, \\(Y\\) is binary, taking a value of 1 to indicate the occurrence of an event or 0 to indicate its absence. Therefore, since there is no longer a meaningful attribution of unexplained and explained variance in the dependent variable, \\(R^2\\) can no longer be interpreted as the percent of variance explained by the model. Similarly to linear regression, residuals, \\(\\varepsilon_i\\), are calculated as the difference between the observed values of the dependent variable , \\(y_i\\), and the predicted values of the dependent variable, \\(\\hat{y}_i\\): \\[\n\\varepsilon_i = y_i - \\hat{y}_i\n\\] In logistic regression, however, the predicted values, \\(\\hat{y}_i\\), represent the probability that \\(Y=1\\) , while \\(y_i\\) represent the binary outcome (\\(Y=1\\) or \\(Y=0\\)). Thus, residuals represent the difference between the observed binary outcome and the model’s predicted probabilities. Theoretically a model of good fit predicts high probabilities of \\(Y=1\\) if \\(y_i\\) actually equals 1 and a low probability of \\(Y=1\\) if \\(y_i\\) is actually 0. In order to determine what is considered high probability and low probability, a cut-off value is imposed on the \\(\\hat{y}_i\\) values. Cut-off values are then evaluated based on their specificity, sensitivity, and misclassification rates. Sensitivity, also called the true positive rate, is the proportion of actual positives that are correctly identified: \\[\n\\text{Sensitivity} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\] In this analysis, the sensitivity rate is the proportion of observed \\(y_i\\) = 1 values correctly predicted as 1. Specificity, also called the true negative rate, is the proportion of actual negatives that are correctly identified as negatives: \\[\n\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}}\n\\] The specificity rate in this analysis is the proportion of observed \\(y_i\\) = 0 values correctly predicted as 0. The misclassification rate is the proportion of incorrectly identified positive and negative \\(y_i\\) values based on the total number of predictions: \\[\n\\text{Misclassification} = \\frac{\\text{False Negatives}+ \\text{False Positives}}{\\text{True Negatives} + \\text{True Positives} + \\text{False Positives} +  \\text{False Negatives}}\n\\] In R, we called upon fit.binary and set the fit parameter to various different values to simulate how various cut off values would impact the sensitivity, specification, and misclassification rate. In other words, we use multiple cut-off values to compare the trade-offs of each cut-off threshold. Ideally, the chosen threshold will achieve higher sensitivity and specificity while minimizing the misclassification rate.\nReceiver Operating Characteristics (ROC) curves are another tool for evaluating cut-off values. The ROC curve plots sensitivity against the false positive rate (1 – specificity) across all possible cut-off values of \\(\\hat{y}_i\\). The baseline for evaluating ROC curves called the “worthless” ROC is a 45 degree line where sensitivity and the false positive rate are equal across all cut-off values, meaning the predictions are no better than a random guess. Effective models produce ROC curves that lie above this diagonal baseline. ROC curves can be used to determine the cut-off value that balances the sensitivity and specificity rate, characteristics that indicate a good model. One common way to determine the optimal cut-off value is to use the Youden Index, which identifies the cut-off that maximizes the sum of sensitivity and specificity is maximized: \\[\nJ = Sensitivity + Specificity - 1\n\\] This corresponds to the point on the ROC curve farthest above the diagonal line, or equivalently, the point closest to the top-left corner of the graph where sensitivity and specificity both equal 1. To identify the optimal cut‑off value, we implemented a function in R that is conceptually similar to the Youden Index as it attempts to find the point that minimizes the distance to this ideal point.\nIn addition to identifying an optimal cut‑off, we can also calculate Area Under Curve (AUC) for our ROC curve as a measure of the model’s overall predictive accuracy. The AUC quantifies the model’s ability to discriminate between positive and negative outcomes across all possible cut‑offs. An AUC of 1 (area of the entire graph) indicates perfect classification or discrimination while a value of 0.5 (area under the 45 degree line) indicates no better than random guessing. AUC can be interpreted as the probability that the model assigns a higher predicted probability to a randomly chosen positive case than to a randomly chosen negative case. Higher AUC values therefore reflect stronger overall discriminative ability across all possible cut‑off values, implying that at least one threshold exists where both sensitivity and specificity are relatively high. In this analysis, the AUC was computed in R using the performance function from the ROCR package. We relied on commonly established thresholds for evaluating model accuracy based on AUC values where 0.90–1.00 indicates excellent accuracy, 0.80–0.90 good, 0.70–0.80 fair, 0.60–0.70 poor, and 0.50–0.60 indicates the model failed.\nAnother measure used to evaluate logistic regression model fit is the Akaike Information Criterion (AIC). Although the absolute value of the AIC is not interpretable on its own, it provides a basis for comparing two or more models. Specifically, AIC combines the log‑likelihood of the predicted probabilities with a penalty for the number of estimated parameters. Lower AIC values indicate a more favorable balance between model complexity and goodness of fit."
  },
  {
    "objectID": "HW_3/Homework3.html#assumptions-of-logistic-regression",
    "href": "HW_3/Homework3.html#assumptions-of-logistic-regression",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "2.4 Assumptions of Logistic Regression",
    "text": "2.4 Assumptions of Logistic Regression\nLogistic regression models the relationship between a set of predictors and a binary dependent variable by expressing the log-odds of the outcome as a linear function of the predictors:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) =\n\\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k.\n\\]\nThe predicted probability of the outcome is obtained by applying the logistic transformation:\n\\[\np = \\frac{e^{\\eta}}{1 + e^{\\eta}}, \\qquad\n\\eta = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_kX_k.\n\\] Here, \\(p\\) represents the predicted probability of the event occurring, \\(X_k\\) are the predictor variables, and \\(\\beta_k\\) are the corresponding regression coefficients. This formulation ensures that predicted probabilities remain between 0 and 1, while allowing the model to use a linear combination of predictors on the log-odds scale.\n\n2.4.1 Logistic Regression Assumptions\nLogistic regression shares several assumptions with Ordinary Least Squares (OLS) regression, while relaxing others. As in OLS, logistic regression assumes that observations are independent of one another. Independence ensures that the estimated coefficients and their standard errors are valid. The model also assumes that the predictors are not perfectly collinear. Severe multicollinearity inflates standard errors and reduces the reliability of coefficient estimates.\nHowever, several OLS assumptions do not apply to logistic regression. Logistic regression does not assume homoscedasticity of residuals, because the variance of a binary dependent variable is a function of its mean. The model also does not require that residuals follow a normal distribution. In addition, the model does not assume a linear relationship between the predictors and the outcome on the original probability scale. Instead, it assumes linearity only in the log-odds, as shown in the equations above."
  },
  {
    "objectID": "HW_3/Homework3.html#exploratory-analyses-prior-to-logistic-regression",
    "href": "HW_3/Homework3.html#exploratory-analyses-prior-to-logistic-regression",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "2.5 Exploratory Analyses Prior to Logistic Regression",
    "text": "2.5 Exploratory Analyses Prior to Logistic Regression\nBefore fitting a logistic regression model, it is useful to assess the relationships among the predictors to ensure that they do not exhibit multicollinearity. To do this, Pearson correlation coefficients can be calculated between the continuous predictors and the dependent variable, as well as among the predictors themselves. Examining the magnitude of these correlations helps determine whether any predictors are highly correlated, which could inflate standard errors and affect coefficient stability in the logistic regression model.\n\n2.5.1 Cross-tabulations for Binary Predictors\nWhen both the dependent variable and a predictor are categorical, a cross-tabulation provides a simple way to examine the distribution of outcomes across different categories of the predictor. To formally test whether the distribution of the dependent variable varies across levels of a binary predictor, the appropriate statistical method is the Chi-Square (\\(\\chi^2\\)) test of independence.\nFor the \\(\\chi^2\\) test, the null hypothesis states that the two categorical variables are independent; that is, the proportion of positive and negative outcomes is the same for both levels of the predictor. The alternative hypothesis states that the variables are not independent, meaning that the distribution of the dependent variable differs across categories of the predictor. A large \\(\\chi^2\\) statistic and a p-value below the conventional significance threshold (e.g., 0.05) provide evidence against the null hypothesis and suggest that an association exists between the two categorical variables.\n\n\n2.5.2 Comparing Means of Continuous Predictors\nFor continuous predictors, it is often useful to compare their mean values across the two categories of the binary dependent variable. The appropriate statistical test for comparing the means of a continuous variable between two independent groups is the independent samples t-test.\nFor the t-test, the null hypothesis states that the mean value of the continuous predictor is the same across both groups of the dependent variable. The alternative hypothesis states that the means differ between the two groups. A large absolute value of the t-statistic and a p-value below the specified significance level (e.g., 0.05) provide evidence to reject the null hypothesis and conclude that there are significant differences in mean values between the groups."
  },
  {
    "objectID": "HW_3/Homework3.html#exploratory-analysis",
    "href": "HW_3/Homework3.html#exploratory-analysis",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "3.1 Exploratory Analysis",
    "text": "3.1 Exploratory Analysis\nBefore beginning the logistic regression, we must do some exploratory analysis of the data and check assumptions of the regression model. Below is a summary table of the dependent variable, displaying both the count and proportion of crashes that involved a drinking driver versus those that did not.\n\n\n\nDistribution of DRINKING_D (Drunk Driving Indicator)\n\n\nDRINKING_D\nCount\nProportion\n\n\n\n\n0\n40879\n0.943\n\n\n1\n2485\n0.057\n\n\n\n\n\nThe distribution of the dependent variable shows that the vast majority of crashes did not involve a drinking driver: 40,879 crashes (94.3%). Only 2,485 crashes, or about 5.7%, involved a drinking driver.\nIt is also useful to examine the relationships between the dependent variable, DRINKING_D, and each of the binary predictors. Table 2 presents the cross-tabulations of DRINKING_D with each predictor, along with the proportion of crashes in each category. For each predictor, the table also includes the Chi-Square p-value to indicate whether the distribution of drinking-driver crashes differs significantly across its categories.\n\n\n\nCross-Tabulation of DRINKING D with Binary Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo Alcohol Involved\n(DRINKING D = 0)\n\n\nAlcohol Involved\n(DRINKING D = 1)\n\n\n\n\nPredictor\nNum\nPct.\nNum\nPct.\nTotal\nChi-square p-value\n\n\n\n\nFATAL_OR_M\n1181\n2.89%\n188\n7.57%\n1369\n&lt;0.001\n\n\nOVERTURNED\n612\n1.50%\n110\n4.43%\n722\n&lt;0.001\n\n\nCELL_PHONE\n426\n1.04%\n28\n1.13%\n454\n0.763\n\n\nSPEEDING\n1261\n3.08%\n260\n10.46%\n1521\n&lt;0.001\n\n\nAGGRESSIVE\n18522\n45.31%\n916\n36.86%\n19438\n&lt;0.001\n\n\nDRIVER1617\n674\n1.65%\n12\n0.48%\n686\n&lt;0.001\n\n\nDRIVER65PLUS\n4237\n10.36%\n119\n4.79%\n4356\n&lt;0.001\n\n\n\n\n\nThe Chi-Square tests indicate whether there is a significant association between DRINKING_D and each binary predictor. For most predictors (FATAL_OR_M, OVERTURNED, SPEEDING, AGGRESSIVE, DRIVER1617, and DRIVER65PLUS) the p-values are less than 0.001, which is far below our significance threshold of 0.05. This allows us to reject the null hypothesis of independence for these variables, suggesting that the occurrence of a drinking-driver crash is significantly associated with these factors.\nIn contrast, the p-value for CELL_PHONE is 0.763, well above 0.05, indicating that we fail to reject the null hypothesis. There is no statistically significant association between drinking-driver crashes and whether the driver was using a cell phone at the time of the crash.\nOverall, these results suggest that most of the binary predictors are significantly related to the likelihood of a crash involving a drinking driver, except for CELL_PHONE.\nTo further explore factors associated with drinking-driver crashes, we next examine the continuous predictors, PCTBACHMOR and MEDHHINC, comparing their means and standard deviation across crashes with and without alcohol involvement and conducting independent samples t-tests,\n\n\n\nSummary of Continuous Predictors by DRINKING D\n\n\n\n\n\n\n\n\n\n\n\n\nNo Alcohol Involved\n(DRINKING D = 0)\n\n\nAlcohol Involved\n(DRINKING D = 1)\n\n\n\n\nPredictor\nMean\nSD\nMean\nSD\nt-test p-value\n\n\n\n\nPCTBACHMOR\n16.56986\n18.21426\n16.61173\n18.72091\n0.914\n\n\nMEDHHINC\n31483.05472\n16930.10159\n31998.75292\n17810.49735\n0.160\n\n\n\n\n\nThe summary statistics for the continuous predictors show that the mean percentage of individuals with a bachelor’s degree or higher (PCTBACHMOR) is very similar between crashes with no alcohol involvement (16.57%) and those with alcohol involvement (16.61%). The independent samples t-test yields a p-value of 0.914, which is far above our significance threshold of 0.05. This indicates that we fail to reject the null hypothesis, suggesting no significant difference in PCTBACHMOR between the two groups.\nSimilarly, the mean median household income (MEDHHINC) is slightly higher for alcohol-involved crashes ($31,998) compared to non-alcohol-involved crashes ($31,483), but the t-test p-value of 0.160 indicates that this difference is not statistically significant. Again, we fail to reject the null hypothesis, implying that MEDHHINC is not significantly associated with the likelihood of a crash involving a drinking driver.\nOverall, the t-test results suggest that neither of the continuous predictors shows a significant association with DRINKING_D in this dataset."
  },
  {
    "objectID": "HW_3/Homework3.html#logistic-regression-assumption-checks",
    "href": "HW_3/Homework3.html#logistic-regression-assumption-checks",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "3.2 Logistic Regression Assumption Checks",
    "text": "3.2 Logistic Regression Assumption Checks\nAs previously mentioned, a key assumption of logistic regression is that there is no severe multicollinearity between predictors. We attempted to test whether our data violated this assumption by creating a pairwise Pearson coefficient matrix that included all our predictors.\n\n\n\n\n\nTable 4 presents the Pearson correlation coefficients between all binary and continuous predictors. Pearson coefficients, \\(r\\), range from 1 to -1 and can be interpret as 1 indicating strong positive linear correlation, -1 indicating strong negative linear correlation, and 0 indicating no linear correlation. Because correlation coefficients are rarely perfectly negative or positive, the threshold considered to indicate moderate correlation is an absolute value of \\(r\\) (\\(|r|\\)) between 0.5 and 0.8 while \\(|r|\\) &lt; 0.5 indicates weak correlation and \\(|r|\\) &gt; 0.8 indicates strong correlation.\nThe coefficients that include binary variables are all uniformly small, near‑zero values. Because the Pearson correlation coefficient is designed to measure the strength of a linear relationship between continuous variables, it is not an ideal measure of association when applied to binary predictors. As a result, Pearson correlation is not as accurate for assessing relationships between binary predictors (or between binary and continuous predictors) and may misrepresent the true association. This means that in our analysis, while the Pearson coefficients are uniformly small, we must be cautious or explore an alternative method in interpreting them.\nThe correlation coefficient between the two continuous variables, PCTBACHMOR and MEDHHINC, is, as expected, the highest as \\(r\\)=0.478. Multicollinearity is considered to occur when two or more predictors are very strongly correlated (\\(|r|\\) &gt; 0.9). The correlation between PCTBACHMOR and MEDHHINC is far below this threshold, suggesting no severe multicollinearity."
  },
  {
    "objectID": "HW_3/Homework3.html#logistic-regression-results",
    "href": "HW_3/Homework3.html#logistic-regression-results",
    "title": "Homework 3: The Application of Logistic Regression to Examine the Predictors of Car Crashes Caused by Alcohol",
    "section": "3.3 Logistic Regression Results",
    "text": "3.3 Logistic Regression Results\n\n3.3.1 Logistic Regression with All Predictors\nTo identify the predictors associated with alcohol-related crashes, we estimated a logistic regression model that included all binary and continuous predictors. The estimated coefficients, p-values, and odds ratios are presented as shown below.\n\n\n\n\n\nThe logistic regression model includes all binary and continuous predictors. This model helps identify which factors are associated with crashes that involve alcohol. Table X presents the estimated coefficients, standard errors, p-values, odds ratios, and 95% confidence intervals for each predictor.\nSeveral predictors are statistically significant. Crashes that resulted in a fatality or major injury show higher odds of involving alcohol, with an odds ratio of approximately 2.26. Crashes involving overturned vehicles are also more likely to involve alcohol, with an odds ratio of about 2.53. Speeding has the strongest association among all predictors, with an odds ratio of approximately 4.66, indicating substantially higher odds of alcohol involvement. Aggressive driving is negatively associated with alcohol involvement, with an odds ratio of 0.55. Both age-related indicators are significant: crashes involving 16–17-year-old drivers or drivers aged 65 or older have lower odds of involving alcohol, with odds ratios of 0.28 and 0.46, respectively.\nTwo predictors are not statistically significant in this model. Cell phone use does not show a meaningful association with alcohol involvement. The percentage of residents with at least a bachelor’s degree (PCTBACHMOR) is also non-significant, with an odds ratio close to 1. Median household income is statistically significant but has an odds ratio of 1.0000028, indicating a negligible substantive effect.\nOverall, the model indicates that specific crash characteristics (fatality, overturning, and speeding) and driver demographics (teenage or senior drivers) are important predictors of alcohol involvement, while cell phone use and neighborhood-level sociodemographic variables contribute little additional explanatory power.\n\n\n3.3.2 Sensitivity, Specificity, Misclassification\nTo evaluate model performance across different probability thresholds, we computed sensitivity, specificity, and the overall misclassification rate for each cut-off value. The results are summarized as shown below.\n\n\n\nSensitivity, Specificity, and Misclassification Rates Across Probability Cut-offs\n\n\nCutoff\nTP\nFN\nTN\nFP\nSensitivity\nSpecificity\nMisclassification\n\n\n\n\n0.02\n2444\n41\n2374\n38505\n0.984\n0.058\n0.889\n\n\n0.03\n2437\n48\n2613\n38266\n0.981\n0.064\n0.884\n\n\n0.05\n1826\n659\n19176\n21703\n0.735\n0.469\n0.516\n\n\n0.07\n550\n1935\n37356\n3523\n0.221\n0.914\n0.126\n\n\n0.08\n459\n2026\n38370\n2509\n0.185\n0.939\n0.105\n\n\n0.09\n418\n2067\n38670\n2209\n0.168\n0.946\n0.099\n\n\n0.10\n408\n2077\n38762\n2117\n0.164\n0.948\n0.097\n\n\n0.15\n259\n2226\n39743\n1136\n0.104\n0.972\n0.078\n\n\n0.20\n57\n2428\n40690\n189\n0.023\n0.995\n0.060\n\n\n0.50\n4\n2481\n40875\n4\n0.002\n1.000\n0.057\n\n\n\n\n\nTo further evaluate model performance, predicted probabilities were converted to binary classifications using a series of probability cut-offs ranging from 0.02 to 0.50. For each cut-off, a confusion matrix was generated, allowing the calculation of sensitivity, specificity, and the overall misclassification rate.\nSensitivity measures the proportion of alcohol-related crashes correctly identified by the model, while specificity measures the proportion of non–alcohol-related crashes correctly classified. The misclassification rate reflects the overall proportion of incorrect predictions. As expected, lower cut-off values result in higher sensitivity and lower specificity, while higher cut-offs reverse this pattern.\nAcross the tested cut-offs, the lowest misclassification rate occurred at a cut-off of 0.05, indicating that this threshold achieves the best balance between false positives and false negatives for this model. In contrast, extremely low or high cut-offs, such as 0.02 or 0.50, produce substantially higher misclassification rates and therefore perform less effectively as classification rules.\nThese results illustrate the importance of assessing multiple probability thresholds when evaluating logistic regression models. The optimal choice of cut-off depends on the tradeoff between sensitivity and specificity that is most appropriate for the application.\n\n\n3.3.3 ROC curve & optimal cutoff\nTo evaluate the model’s ability to distinguish between alcohol-related and non–alcohol-related crashes, we generated a receiver operating characteristic (ROC) curve using the predicted probabilities from the full logistic regression model. The ROC curve shows the tradeoff between sensitivity and specificity across all possible probability thresholds, as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\nOptimal Cut-off Value Based on Minimum Distance to (0,1) on the ROC Curve\n\n\n\nSensitivity\nSpecificity\nCutoff\n\n\n\n\nsensitivity\n0.661\n0.545\n0.0637\n\n\n\n\n\nUsing the distance-to-(0,1) criterion, we identified the probability cut-off that minimizes the distance to the upper-left corner of the ROC space. The optimal cut-off derived from this approach was approximately 0.06365. This value can be compared with the cut-off of 0.05 identified earlier as the point that yielded the lowest misclassification rate. The difference between these two thresholds reflects the fact that the ROC-based method jointly considers sensitivity and specificity, while the misclassification-based approach evaluates only the proportion of incorrect predictions. Because the two criteria optimize different aspects of model performance, they do not necessarily produce the same probability cut-off.\n\n\n3.3.4 Area Under the Curve (AUC)\nThe area under the ROC curve (AUC) provides a summary measure of the model’s overall discriminative ability. The AUC for this model was 0.6399, as shown below, indicating modest ability to distinguish between alcohol-related and non–alcohol-related crashes. An AUC value of 0.5 suggests no discriminatory power, while values above 0.7 are typically considered acceptable. Thus, while the model performs better than random chance, its ability to accurately classify crashes based on alcohol involvement is limited.\n\n\n\nArea Under the ROC Curve (AUC)\n\n\nAUC\n\n\n\n\n0.63987\n\n\n\n\n\n\n\n3.3.5 Reduced model\nTo assess whether the continuous predictors contributed meaningfully to model performance, we estimated a reduced logistic regression model that included only the binary predictors. The estimated coefficients and odds ratios for this reduced model are presented as shown below.\n\n\n\n\n\n\n\n\nAIC Comparison of Models\n\n\nModel\nDf\nAIC\n\n\n\n\nfull_logit\n10\n18359.63\n\n\nbinary_logit\n8\n18360.47\n\n\n\n\n\nThe results of the binary-only model are largely consistent with the full model. Fatal or major injury crashes, overturned vehicles, and speeding remain strong positive predictors of alcohol involvement. Aggressive driving continues to show a negative association. Both age-related predictors—drivers aged 16–17 and drivers aged 65 or older—also remain significant and retain similar effect sizes. As in the full model, cell phone use is not a significant predictor of alcohol involvement.\nComparing the reduced model with the full model shows that removing the continuous predictors does not change the significance of any of the key crash-related or demographic variables. However, the full model has a slightly lower Akaike Information Criterion (AIC = 18359.63) than the reduced model (AIC = 18360.47). Because lower AIC values indicate better model fit, this comparison suggests that the full model provides a marginally better fit, even though the continuous predictors do not substantially alter the significance or magnitude of the main effects."
  },
  {
    "objectID": "HW_6/Homework6.html",
    "href": "HW_6/Homework6.html",
    "title": "Homework 6: IMDB Text Mining & Sentiment Analysis",
    "section": "",
    "text": "In this analysis, we performed text-mining techniques to movie reviews from the Internet Movie Database (IMDb) in order to quantify and visualize word trends and emotional tones across reviews. Text-mining combines data cleaning and language processing techniques, enabling researchers to systematically analyze unstructured text for meaningful patterns such as term frequency and emotional sentiments. This approach combines the ease of decreased manual effort with nuance in understanding narratives and perspectives."
  },
  {
    "objectID": "HW_6/Homework6.html#data-preprocessing",
    "href": "HW_6/Homework6.html#data-preprocessing",
    "title": "Homework 6: IMDB Text Mining & Sentiment Analysis",
    "section": "2.1 Data Preprocessing",
    "text": "2.1 Data Preprocessing\nWe began by importing the IMDb dataset csv into R and converting the review column into a corpus object using the tm package. The VectorSource function was called in order to treat every review as a separate document. The result was a corpus which, in this context, streamlines analysis by serving as a repository of the text documents. The corpus was then preprocessed to ensure uniformity and remove noise. Specifically, all entries were transformed to lowercase, numbers and punctuation were removed, and common English stopwords were excluded. In addition, a small set of self-defined stop and non-english words (“I”, “br”, “You,”, “The”, “A”, “It”) were removed after additional data exploration to further reduce noise."
  },
  {
    "objectID": "HW_6/Homework6.html#word-cloud-creation",
    "href": "HW_6/Homework6.html#word-cloud-creation",
    "title": "Homework 6: IMDB Text Mining & Sentiment Analysis",
    "section": "2.2 Word Cloud Creation",
    "text": "2.2 Word Cloud Creation\nAfter preprocessing we created a document term matrix (DTM) which represents the frequency of terms across all documents. In this matrix, each row corresponds with a document, each column corresponds to a unique term, and the cell values represent the number of times the term itself appears in a given document.\nFrom this DTM, two visuals were created to represent the frequency of terms: a histogram and a word cloud. In a word cloud visualization, words are displayed in font sizes proportional to the their frequency, allowing words repeated more frequently to be more visible. We used the wordcloud function to create our visual which takes a specified threshold for which words to display based on frequency. In analysis, we chose to only display words that appeared more than 500 times to ensure variation but also to reduce noise."
  },
  {
    "objectID": "HW_6/Homework6.html#sentiment-analysis",
    "href": "HW_6/Homework6.html#sentiment-analysis",
    "title": "Homework 6: IMDB Text Mining & Sentiment Analysis",
    "section": "2.3 Sentiment Analysis",
    "text": "2.3 Sentiment Analysis\nSentiment analysis was conducted to identify the emotional tone present within the IMDb review corpus. After converting the cleaned text into a document-term matrix, we used the NRC Emotion Lexicon, implemented through the syuzhet package, to map individual terms to ten emotion categories: anger, anticipation, disgust, fear, joy, sadness, surprise, trust, positive, and negative. A lexicon-based approach was chosen because it provides interpretable emotional classifications and is well suited for short and medium-length texts such as movie reviews.\nTo generate sentiment scores, we first extracted all terms appearing in a selected review along with their corresponding frequencies. Each word was then matched against the NRC lexicon to determine which emotions it contributes to. Because words may appear multiple times within the same review, emotional scores were weighted by term frequency to more accurately capture the intensity of expressed sentiment. After multiplying each emotion indicator by word frequency, we summed the results across all terms to produce an aggregate sentiment profile for the review. This process allowed us to quantify the emotional composition of the text and visualize it through a barplot illustrating the relative prominence of different emotions. Overall, the sentiment analysis approach provides a straightforward and interpretable way to characterize how reviewers express feelings toward the films they describe."
  },
  {
    "objectID": "HW_6/Homework6.html#word-cloud",
    "href": "HW_6/Homework6.html#word-cloud",
    "title": "Homework 6: IMDB Text Mining & Sentiment Analysis",
    "section": "3.1 Word Cloud",
    "text": "3.1 Word Cloud\n\n3.1.1 Preprocessing and Document-Term Matrix\nAfter loading the IMDb review dataset, we applied a series of preprocessing steps to standardize the text data. These procedures included converting all characters to lowercase, removing numbers and punctuation, and eliminating English stopwords. Additional high-frequency but uninformative tokens such as “I,” “br,” “you,” “the,” “a,” and “it” were removed to ensure that the final corpus emphasized meaningful, content-bearing words. This process resulted in clean textual entries consisting primarily of descriptive terms and phrases relevant to the reviews.\n\n\n\nExamples of Original and Cleaned IMDb Reviews\n\n\nDocument_ID\nOriginal_Text\nCleaned_Text\n\n\n\n\n1\nOne of the other reviewers has mentio...\none reviewers mentioned watchin...\n\n\n2\nA wonderful little production. &lt;br /&gt;...\nwonderful little production filmi...\n\n\n3\nI thought this was a wonderful way to...\nthought wonderful way spend time...\n\n\n\n\n\nA document-term matrix (DTM) was then constructed to quantify the frequency of each term across the corpus. The DTM contains 199 documents and 7,937 unique terms with an overall sparsity of 99%, which is typical of natural language data. Inspecting sample rows of the matrix reveals that commonly used film-related words—such as “film,” “movie,” “story,” and “good”—appear across many reviews, whereas most other terms occur only once or a few times. This reinforces the long-tailed nature of the dataset, where a small number of general descriptive words dominate, and thousands of low-frequency terms reflect specific opinions or contexts unique to individual reviews.\n\n\n\nDocument-Term Matrix Summary\n\n\nMetric\nValue\n\n\n\n\nNumber of Documents\n199\n\n\nNumber of Terms\n7870\n\n\nNon-zero Entries\n20454\n\n\nSparsity\n98.69%\n\n\n\n\n\n\n\n3.1.2 Term Frequency Distribution\nThe histogram of term frequencies further illustrates this pattern: the vast majority of words fall into the lowest frequency bin, with only a small number appearing more than 20 or 30 times. Such a distribution is expected in movie reviews, where each author introduces unique vocabulary while still relying on a shared set of evaluative and narrative descriptors. This distribution directly influences the terms that dominate the word cloud visualization.\n\n\n\n\n\n\n\n\n\n\n\n3.1.3 Word Cloud\nThe word cloud highlights the terms that occur most frequently across the corpus. Larger words, such as “many,” “performance,” “plot,” “game,” “classic,” and “hotel,” indicate repeated appearance in reviews and reflect common themes related to storytelling, acting quality, and genre conventions. Additionally, expressive adjectives such as “difficult,” “appropriate,” “fabulous,” and “unexpectedly” suggest that reviewers frequently rely on emotional and descriptive language to articulate their reactions to the films.\nOverall, the combination of the preprocessing results, term frequency distribution, and word cloud visualization offers a coherent picture of the lexical patterns present in the dataset. These findings demonstrate the diversity of vocabulary used by reviewers and highlight the central descriptive themes that recur throughout the IMDb corpus."
  },
  {
    "objectID": "HW_6/Homework6.html#sentiment-analysis-1",
    "href": "HW_6/Homework6.html#sentiment-analysis-1",
    "title": "Homework 6: IMDB Text Mining & Sentiment Analysis",
    "section": "3.2 Sentiment Analysis",
    "text": "3.2 Sentiment Analysis\nThe sentiment analysis was conducted on the first review in the IMDB dataset using the NRC lexicon, which categorizes words into eight emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust) and two sentiments (negative, positive). Each term in the review was matched against the NRC sentiment dictionary, and the sentiment scores were weighted by the frequency of each term’s appearance in the review.\nThe bar plot below displays the total sentiment scores across all ten categories. The analysis shows that negative sentiment dominates this review, with a count of approximately 21 compared to positive sentiment at around 8. Among the emotions, fear, anger, and sadness are most prevalent, while trust is the highest-scoring positive emotion.\n\n\n\n\n\n\n\n\n\nHowever, visual inspection of the actual review reveals it is positive in nature. This highlights a key limitation of lexicon-based sentiment analysis: it does not account for context. The high scores for fear, anger, and sadness likely reflect the reviewer describing the film’s plot — discussing scary or tense scenes — rather than expressing dissatisfaction with the movie. This approach accounts for term frequency but cannot distinguish between describing negative content positively versus expressing genuine criticism of the film."
  },
  {
    "objectID": "index.html#click-the-links-below-to-view-various-musa-5000-reports.",
    "href": "index.html#click-the-links-below-to-view-various-musa-5000-reports.",
    "title": "Welcome",
    "section": "",
    "text": "Note: These reports were originally prepared as PDFs rather than HTML pages. Each page includes a link to view the PDF version, which may be preferable if you encounter any rendering issues in the browser.\n\n\n\nIn this analysis, we use a multiple linear regression model to predict median house values in Philadelphia. Drawing on Philadelphia’s tract-level census data, we examine the impact of our four predictors on our response variable block group median house value: percentage with at least a bachelor’s degree, percentage of vacant spaces, number living below the poverty line, and percentage of single family housing units.\n\n\n\n\n\nHere, we use a set of spatial models to better account for geographic dependence in housing values across Philadelphia. Because housing markets and neighborhood conditions are not independent across space, we apply spatial lag and spatial error models to capture spillover effects between nearby block groups. We also use geographically weighted regression to allow the relationships between housing values and our predictors to vary across the city, helping us understand how these effects differ by location rather than assuming a single citywide relationship.\n\n\n\n\n\nThis analysis examines alcohol-impaired driving crashes in Philadelphia and the factors associated with whether a crash involved a drinking driver. Using crash-level data linked to neighborhood demographics, we look at how crash characteristics, age groups, and surrounding socioeconomic conditions relate to alcohol involvement. We use a logistic regression model to estimate the likelihood that a given crash was alcohol-related and to explore how these relationships vary across the city.\n\n\n\n\n\nIn this analysis, we performed text-mining techniques to movie reviews from the Internet Movie Database (IMDb) in order to quantify and visualize word trends and emotional tones across reviews."
  }
]