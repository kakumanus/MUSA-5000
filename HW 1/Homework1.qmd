---
title: "Homework 1: Using OLS Regression to Predict Median House Values in Philadelphia"
author: "Yiming Cao, Sujan Kakumanu, Angel Sanaa Rutherford"
date: October 15 2025
number-sections: true
format: pdf
---
```{r setup}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
options(scipen = 999)

library(ggplot2)             
library(dplyr)
library(sf)
library(ggplot2)
library(patchwork)
library(MASS)
library(caret)
```

```{r exploratory-data-analysis}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
regression_data <- read.csv("./RegressionData.csv")

hist(regression_data$MEDHVAL)
hist(regression_data$PCTBACHMOR)
hist(regression_data$NBELPOV100)
hist(regression_data$PCTVACANT)
hist(regression_data$PCTSINGLES)

plot(regression_data$PCTBACHMOR, regression_data$MEDHVAL)
plot(regression_data$NBELPOV100, regression_data$MEDHVAL)
plot(regression_data$PCTVACANT, regression_data$MEDHVAL)
plot(regression_data$PCTSINGLES, regression_data$MEDHVAL)

mean_medhval <- mean(regression_data$MEDHVAL)
mean_pctbachmor <- mean(regression_data$PCTBACHMOR)
mean_nbelpov100 <- mean(regression_data$NBELPOV100)
mean_pctvacant <- mean(regression_data$PCTVACANT)
mean_pctsingles <- mean(regression_data$PCTSINGLES)

sd_medhval <- sd(regression_data$MEDHVAL)
sd_pctbachmor <- sd(regression_data$PCTBACHMOR)
sd_nbelpov100 <- sd(regression_data$NBELPOV100)
sd_pctvacant <- sd(regression_data$PCTVACANT)
sd_pctsingles <- sd(regression_data$PCTSINGLES)
```

```{r exploratory-data-analysis-log}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
regression_data$LNMEDHVAL<-log(regression_data$MEDHVAL)

regression_data$LNPCTBACHMOR<-log(1+regression_data$PCTBACHMOR)
regression_data$LNNBELPOV100<-log(1+regression_data$NBELPOV100)
regression_data$LNPCTVACANT<-log(1+regression_data$PCTVACANT)
regression_data$LNPCTSINGLES<-log(1+regression_data$PCTSINGLES)

hist(regression_data$LNMEDHVAL)
hist(regression_data$LNPCTBACHMOR)
hist(regression_data$LNNBELPOV100)
hist(regression_data$LNPCTVACANT)
hist(regression_data$LNPCTSINGLES)

plot(regression_data$PCTBACHMOR, regression_data$LNMEDHVAL)
plot(regression_data$LNNBELPOV100, regression_data$LNMEDHVAL)
plot(regression_data$PCTVACANT, regression_data$LNMEDHVAL)
plot(regression_data$PCTSINGLES, regression_data$LNMEDHVAL)

cor(regression_data$PCTBACHMOR, regression_data$LNMEDHVAL, method="pearson")
cor(regression_data$LNNBELPOV100, regression_data$LNMEDHVAL, method="pearson")
cor(regression_data$PCTVACANT, regression_data$LNMEDHVAL, method="pearson")
cor(regression_data$PCTSINGLES, regression_data$LNMEDHVAL, method="pearson")

trimmed_regression_data <- regression_data %>% dplyr::select(PCTBACHMOR, LNNBELPOV100, PCTVACANT, LNMEDHVAL, PCTSINGLES)
#remove LNMEDHVAL in final product
cor(trimmed_regression_data)
```

```{r exploratory-data-analysis-mapping}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
Regression_shpData <- st_read("./Lecture 1 - RegressionData.shp")

ggplot(Regression_shpData) +
  geom_sf(aes(fill =  LNMEDHVAL), color = NA) +
  scale_fill_viridis_c(option = "plasma")+
  labs(
    title = "Log Transformed Median House Value by Census Block Groups",
    subtitle = "Philadelphia"
  ) +
  theme_minimal()

base_theme <- theme_minimal() +
  theme(
    axis.text = element_blank(),
    axis.title = element_blank(),
    panel.grid = element_blank(),
    plot.title = element_text(size = 6, margin = margin(b = 4))  # smaller + some breathing room
  )

p1 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = PCTVACANT), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Percent of Housing Units that are Vacant,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

p2 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = PCTSINGLES), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Percent of Housing Units that are Single Family Detatched,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

p3 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = PCTBACHMOR), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Percent of Housing Units that have at least Bachelor's Degree,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

p4 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = LNNBELPOV), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Log Transformed Number of Households Below 100% Poverty Level,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

(p1 | p2) / (p3 | p4)
```

```{r multiple-regression-analysis}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
#| 
reg1 <- lm(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV, data=Regression_shpData)

summary(reg1)

anova(reg1)

Regression_shpData$predvals <- fitted(reg1) 

Regression_shpData$resids <- residuals(reg1)

Regression_shpData$stdres <- rstandard(reg1)

plot(Regression_shpData$predvals, Regression_shpData$stdres)

```

```{r step}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
step <- stepAIC(reg1, direction="both")

step$anova
```

```{r k-fold-cv}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
train_control <- trainControl(method = "cv", number = 5)

cv_model <- train(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV,
                  data = Regression_shpData,
                  method = "lm",
                  trControl = train_control)
cv_model

cv_model_2 <- train(LNMEDHVAL ~ PCTVACANT + MEDHHINC,
                  data = Regression_shpData,
                  method = "lm",
                  trControl = train_control)
cv_model_2
```

```{r std-residuals-charts}
#| echo: false
#| results: "hide" # Hide the output
#| message: false  # Hide messages
#| warning: false  # Hide warnings
#| fig.show: "hide" # Hide plots/figures
hist(Regression_shpData$stdres)

ggplot(Regression_shpData) +
  geom_sf(aes(fill =  stdres), color = NA) +
  scale_fill_viridis_c(option = "plasma")+
  labs(
    title = "",
    subtitle = ""
  ) +
  theme_minimal()
```

# Introduction

  In this analysis, we use a multiple linear regression model to predict median house values in Philadelphia. Drawing on Philadelphia‚Äôs tract-level census data, we examine the impact of our four predictors on our response variable median house value: percentage with at least a bachelor‚Äôs degree, percentage of vacant spaces, number living below the poverty line, and percentage of single family housing units.

  Prior theoretical knowledge of the relationships between housing markets and socioeconomic factors has led us to hypothesize a relationship between these four predictors and median house value. High rates of educational attainment and single-family homes are likely positively associated with house values as they may indicate neighborhood stability by signaling higher earning potential and long-term residency. Conversely, high rates of vacancy and poverty levels are likely negatively associated with house values as they may indicate neighborhood instability through a lack of high earning residents and occupants overall. In this analysis, we aim to assess the explanatory power of these predictors and briefly explore if these relationships possess any spatial patterns. 

# Methods

## Data Cleaning
The original dataset contained 1,816 Census block groups across Philadelphia. Following the data preparation protocol provided in the assignment, we removed four types of block groups:  
1. Those with fewer than 40 residents,  
2. Those with no housing units,  
3. Those with median house values below \$10,000, and  
4. One extreme outlier in North Philadelphia with an exceptionally high median house value (over \$800,000) and a very low median household income (below \$8,000).  

After applying these filters, the final cleaned dataset consisted of 1,720 block groups. All variables used in this analysis were numeric and contained no missing values after cleaning.

## Exploratory Data Analysis

### Variable Distributions
We began by importing the cleaned dataset and examining the distributions of the dependent variable (**MEDHVAL**) and four key predictors:  
- **PCTBACHMOR** ‚Äì Percentage of residents with at least a bachelor‚Äôs degree  
- **PCTVACANT** ‚Äì Percentage of housing units that are vacant  
- **PCTSINGLES** ‚Äì Percentage of detached single-family homes  
- **NBELPOV100** ‚Äì Number of households below the poverty line  

```{r exploratory-data-analysis01}
regression_data <- read.csv("./RegressionData.csv")

hist(regression_data$MEDHVAL)
hist(regression_data$PCTBACHMOR)
hist(regression_data$NBELPOV100)
hist(regression_data$PCTVACANT)
hist(regression_data$PCTSINGLES)

plot(regression_data$PCTBACHMOR, regression_data$MEDHVAL)
plot(regression_data$NBELPOV100, regression_data$MEDHVAL)
plot(regression_data$PCTVACANT, regression_data$MEDHVAL)
plot(regression_data$PCTSINGLES, regression_data$MEDHVAL)

mean_medhval <- mean(regression_data$MEDHVAL)
mean_pctbachmor <- mean(regression_data$PCTBACHMOR)
mean_nbelpov100 <- mean(regression_data$NBELPOV100)
mean_pctvacant <- mean(regression_data$PCTVACANT)
mean_pctsingles <- mean(regression_data$PCTSINGLES)

sd_medhval <- sd(regression_data$MEDHVAL)
sd_pctbachmor <- sd(regression_data$PCTBACHMOR)
sd_nbelpov100 <- sd(regression_data$NBELPOV100)
sd_pctvacant <- sd(regression_data$PCTVACANT)
sd_pctsingles <- sd(regression_data$PCTSINGLES)


# Create a tidy summary table
table1 <- tibble(
  Variable = c(
    "Median House Value (MEDHVAL)",
    "Households Below Poverty (NBELPOV100)",
    "% with Bachelor‚Äôs or Higher (PCTBACHMOR)",
    "% Detached Single-Family Homes (PCTSINGLES)",
    "% Vacant Housing Units (PCTVACANT)"
  ),
  Mean = c(66287.73, 189.77, 16.08, 9.23, 11.29),
  SD   = c(60006.08, 164.32, 17.70, 13.25, 9.63)
)

cat("Table 1. Summary statistics for dependent and predictor variables\n")
print(table1, row.names = FALSE)
```

Summary statistics of the dependent and predictor variables are shown in Table 1.


All raw variables exhibited positive skewness, especially MEDHVAL and NBELPOV100, which justified a logarithmic transformation to stabilize variance.

Log Transformations

The natural log of MEDHVAL was computed to improve normality, creating LNMEDHVAL as the dependent variable. For the predictors, only NBELPOV100 benefited from transformation, producing LNNBELPOV100. Other predictors retained strong spikes at zero after log transformation and were thus kept in their original form.
```{r}
regression_data$LNMEDHVAL<-log(regression_data$MEDHVAL)
regression_data$LNNBELPOV100<-log(1+regression_data$NBELPOV100)
regression_data$LNPCTBACHMOR<-log(1+regression_data$PCTBACHMOR)
regression_data$LNPCTVACANT<-log(1+regression_data$PCTVACANT)
regression_data$LNPCTSINGLES<-log(1+regression_data$PCTSINGLES)

hist(regression_data$LNMEDHVAL)
hist(regression_data$LNNBELPOV100)
```
The log transformation substantially improved the symmetry of LNMEDHVAL, which appeared approximately normal. LNNBELPOV100 also showed improvement, whereas the log-transformed percentage variables remained zero-inflated.

Correlation Analysis

To examine potential multicollinearity, Pearson correlation coefficients were calculated among the four predictors. The sample correlation coefficient is defined as:
$$
r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}
         {\sqrt{\sum (x_i - \bar{x})^2}\sqrt{\sum (y_i - \bar{y})^2}}
$$
```{r}
trimmed_regression_data <- regression_data %>% 
  dplyr::select(PCTBACHMOR, LNNBELPOV100, PCTVACANT, PCTSINGLES)

cor(trimmed_regression_data)
```
Correlations were moderate: the strongest being between PCTBACHMOR and LNNBELPOV100 (r = ‚Äì0.32). This indicates that the predictors were not highly collinear and all could be retained in the regression model.

Mapping and Spatial Patterns

To visualize the spatial patterns, choropleth maps were created for LNMEDHVAL and each predictor variable using the shapefile provided.
```{r}
Regression_shpData <- st_read("./Lecture 1 - RegressionData.shp")

ggplot(Regression_shpData) +
  geom_sf(aes(fill = LNMEDHVAL), color = NA) +
  scale_fill_viridis_c(option = "plasma")+
  labs(title = "Log Transformed Median House Value by Census Block Groups", subtitle = "Philadelphia") +
  theme_minimal()

p1 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = PCTVACANT), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Percent of Housing Units that are Vacant,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

p2 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = PCTSINGLES), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Percent of Housing Units that are Single Family Detatched,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

p3 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = PCTBACHMOR), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Percent of Housing Units that have at least Bachelor's Degree,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

p4 <- ggplot(Regression_shpData) +
  geom_sf(aes(fill = LNNBELPOV), color = NA) +
  scale_fill_viridis_c(option = "plasma", na.value = "grey80") +
  labs(title = "Log Transformed Number of Households Below 100% Poverty Level,\n at Block Group", fill = "Value") +
  theme_minimal() +
  theme(axis.text = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) +
  base_theme

(p1 | p2) / (p3 | p4)

```
These maps revealed distinct spatial clustering: higher house values and educational attainment were concentrated in Center City and the northwest, while vacancy and poverty were highest in North and West Philadelphia. The pattern suggested potential spatial dependence, which will be tested in future assignments.

## Multiple Regression Analysis
Multiple regression models a dependent variable as a function of multiple predictors, rather than a single predictor such as in simple regression. These predictors each have a coefficient that represents their effect on a dependent variable, controlling for all other predictors. This approach improves model accuracy in situations where multiple variables better explain outcomes of a dependent variable.

This report regressed log-transformed median house value (LNMEDHVAL) on the proportion of housing units that are vacant (PCTVACANT), percent of housing units that are single family detached homes (PCTSINGLES), proportion of residents with at least a bachelor‚Äôs degree (PCTBACHMOR), and log-transformed number of households with incomes below 100% poverty level (LNNBELPOV). This regression function can be expressed as follows:
$$
\text{LNMEDHVAL} = \beta_0 + \beta_1 \text{PCTVACANT} + \beta_2 \text{PCTSINGLES} + \beta_3 \text{PCTBACHMOR} + \beta_4 \text{LNNBELPOV} + \varepsilon
$$
Multiple regression relies on several key assumptions, most of which mirror the assumptions of simple regression. First, linear relationships should exist between the dependent variable and each predictor, assessed through scatterplots or residual plots and addressed via transformations if needed. Second, residuals should be approximately normally distributed, which can be assessed through a histogram. Third, residuals must be random ‚Äî indicating that observations are not systematically related. Fourth, residuals must be homoscedastic, exhibiting constant variance across all values. Fifth, the dependent variable should be continuous.

A unique assumption for multiple regression is avoiding perfect multicollinearity: no predictor should be strongly correlated with others. Multicollinearity inflates standard errors and produces unstable coefficient estimates. This assumption can be checked by analyzing the correlation coefficients between all dependent variables, with anything greater than 0.9 generally being a cause for concern. Variance Inflation Factor (VIF) can be used to further inspect a suspicion of multicollinearity, with a VIF < 5 being generally acceptable and a VIF < 10 warranting more inspection. A VIF > 10 strongly indicates multicollineariy. 

In the above multiple regression function, $\beta_0$ represents the depedent variable when all predictors are zero. The coefficients of the predictors $\beta_1, \beta_2, \beta_3, \beta_4$ each represent the change in the dependent variable with a one unit increase in the predictor, holding all other predictors constant.

These $\beta$ coefficients in multiple regression are simultaneously estimated in order to minimize the Error Sum of Squares (SSE). The general formula and breakdown of what is to be minimized is provided below (with n being the number of observations, and k is the number of predictors):
$$
SSE = \sum_{i=1}^{n} \varepsilon^2 
     = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
     = \sum_{i=1}^{n} \left[ y_i - \left( \hat{\beta}_0 + \hat{\beta}_1 x_{1i} + \hat{\beta}_2 x_{2i} + \cdots + \hat{\beta}_k x_{ki} \right) \right]^2
$$

This minimization works by finding the $\beta$ coefficients that, when raw predictor $(x_{i})$ data is used, will minimize the residuals $(y_i - \hat{y}_i)$. SSE is also used to calculate Mean Squared Error (MSE), noted by the estimated parameter $\hat\sigma^2$. This is the estimate of the variance of the error term $\epsilon$. The formula for MSE, in terms of SSE is noted below:
$$
MSE = \frac{SSE}{n - (k+1)}
$$

Another term in regression analysis is Total Sum of Squares (SST). It measures the total variation in the dependent variable around it's mean by using the following formula:
$$
SST = \sum_{i=1}^{n} (y_i - \bar{y})^2
$$
Using this formula for SST, and the previously stated formula for SSE, we can calculate $R^2$ ‚Äî the coefficient of multiple determination. This is the proportion of variance in the model explained by all k predictors, and is the represented by the following:
$$
R^2 = 1 - \frac{\text{SSE}}{\text{SST}}
$$
Multiple regression presents a unique dillema in comparison to simple regression, in that adding more predictors will generally increase $R^2$. Adjusting $R^2$, noted below, can account for additional predictors and determine whether or not they are improving the model.
$$
R_{\text{adj}}^2 = \frac{(n-1) R^2 - k}{n - (k+1)}
$$
This report will conduct two tests to evaluate the model. First, there is the F-ratio ‚Äî a model utility test. F-ratio tests the following null hypothesis $H_0$ and alternative hypothesis $H_a$:
$$
H_0: \beta_1 = \beta_2 = \beta_3 = \beta_4 = 0
$$
$$
H_a: \text{At least one } \beta_i \neq 0
$$
In essence, the null hypothesis states that all of the model $\beta$ parameters are zero, and the alternative states that at least one of those parameters is not zero. Failure to reject the null hypothesis suggests that the model is incredibly weak, and should be reevaluated. If the null hypothesis is rejected, the second test can be conducted with the following hypotheses.
$$
H_0: \beta_i = 0
$$
$$
H_a: \beta_i \neq 0
$$
In this test, we evaluate the performance of each predictor i (in the case of this report, the 4 predictors stated earlier). A t-test can be conducted, where the t-statistic for each predictor is calculated as the estimated coefficient divided by its standard error:
$$
t_i = \frac{\hat{\beta}_i - \beta_i}{s_{\hat{\beta}_i}}
$$
Each predictor has its own p-value calculated using the above t-statistic. If the p-value is less than 0.05, we reject the null hypothesis for that predictor and conclude that it is a statistically significant predictor of the dependent variable. If the p-value is greater than or equal to 0.05, we fail to reject the null hypothesis and conclude that the predictor is not statistically significant.


## Additional Analysis

###
  Using the stepAIC() and step$anova command, we applied bidirectional stepwise regression to analyze the fit of our linear model. Stepwise regression determines the minimum number of predictors that yield the best model. Stepwise regression automatically selects or eliminates predictors, either forwards, backwards, or bidirectionally, based on some type of criteria that measures the goodness of fit. In this case, we are attempting to determine the predictor or combination of predictors that minimize the Akaike Information Criterion (AIC).
  
  Stepwise regression, however, poses many limitations as it does not consider theoretical relevance of the predictors, may overlook alternative valid models, and runs the risk of excluding important predictors and including unimportant predictors, especially due to the numerous t-tests measuring whether the null hypothesis,  $\beta_k$ = 0, is true.

###
  To perform cross-validation, we used the trainControl() function with the method parameter set to ‚Äúcv‚Äù (cross-validation) and the train() function with the method parameter set to ‚Äúlm‚Äù (linear model). Cross-validation is a technique that measures model performance unbiasedly by training the model on a select subgroup of observations and seeing how well it estimates deliberately excluded observations. K-fold cross validation where k=5, specifically, divides data sets into five non-overlapping folds and repeatedly uses four folds for training the model and one fold for validating the model so that each fold trains the model multiple times and validates the model once. This method ensures a model‚Äôs generalizability to new data and minimizes distortion by avoiding omitting and duplicating data in its measure of fit.  After all five iterations are complete, the Root Mean Squared Error (RMSE) of the model is returned as a measurement of the average magnitude of predicted residuals or errors between predicted values, $\hat{ùë¶}_i$, of observation $i$, estimated by the model‚Äôs $\beta$ coefficient, and the actual value, $y_i$, of the validation set. The complete formula for RMSE is as follows: 
$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$
After performing k-fold cross-validation on two or more models, the RMSE of the models can be compared to determine which model has the best performance. A smaller RMSE indicates that the model‚Äôs predictions are, on average, closer to the actual values, and thus more representative of the data.

## Software Used

###
  All data analysis was conducted using R. Within R, the following packages were used to perform data preparation, exploratory analysis, regression modeling, and visualization: ggplot, dplyr, sf, patchwork, MASS, and caret. 
 
# Results

## Exploratory Results


Table 1 summarizes the descriptive statistics for all key variables. LNMEDHVAL exhibited approximately normal distribution after transformation, while NBELPOV100 was best represented in logarithmic form. The percentage-based predictors remained right-skewed but were retained due to interpretability.

The correlation matrix indicated moderate relationships among predictors, with no evidence of severe multicollinearity. LNMEDHVAL correlated most strongly with PCTBACHMOR (r = 0.736) and negatively with PCTVACANT (r = ‚Äì0.514) and LNNBELPOV100 (r = ‚Äì0.424), confirming the visual impressions from the choropleth maps.

The spatial maps (Figure 1) clearly illustrated neighborhood-level variation in socioeconomic conditions.

High-value neighborhoods overlapped with areas of high educational attainment.

Poverty and vacancy rates were concentrated in North and West Philadelphia.

Detached single-family housing was more common in outer and suburban-edge block groups.

These exploratory results collectively indicate that higher educational attainment and single-family housing shares are positively associated with housing values, whereas higher vacancy and poverty rates are negatively associated. This provides a strong theoretical and empirical foundation for the multiple regression analysis that follows.




## Regression Results

The output of the regression model ($\text{LNMEDHVAL} = \beta_0 + \beta_1 \text{PCTVACANT} + \beta_2 \text{PCTSINGLES} + \beta_3 \text{PCTBACHMOR} + \beta_4 \text{LNNBELPOV} + \varepsilon$) in R is as follows.
```{r}
#| echo: false
#| message: false  # Hide messages
#| warning: false  # Hide warnings
summary(reg1)
```
We regressed the natural log of median house value (LNMEDHVAL) on the percentage of vacant houses (PCTVACANT), percentage of single-family houses (PCTSINGLES), percentage of residents with a bachelor‚Äôs degree or higher (PCTBACHMOR), and the natural log of the neighborhood poverty rate (LNNBELPOV). All four predictors are statistically significant with p-values far below a threshold of p < 0.05.

The log-transformation of median house value (LNMEDHVAL, the dependent variable) means that we can interpret the coefficients as percent changes in median home value for a one unit change in the predictor. A one percentage point increase in vacant houses (PCTVACANT) is associated with an approximate 1.92% decrease in median home value. A one percentage point increase in single-family houses (PCTSINGLES) is associated with an approximate 0.30% increase in median home value. A one percentage point increase in residents with a bachelor‚Äôs degree or higher (PCTBACHMOR) is associated with a roughly 2.09% increase in median home value. For LNNBELPOV ‚Äî a log-transformed predictor ‚Äî a 1% increase in the poverty rate corresponds to an approximate 7.9% decrease in median home value.

The very low p-values indicate that if there were actually no relationship between each predictor and median home value (i.e., $H_0: \beta_i = 0$), the probability of observing the estimated coefficients we see would be very close to zero. Therefore, we can reject the null hypotheses for all predictors $H_0: \beta_i = 0$.

The model explains a substantial portion of the variance in median home values, with $R^2 = 0.6623$ and $R^2_{adj} = 0.6615$. The F-statistic is highly significant, with $F = 840.9 \text{ and a p-value of } p < 0.00000000000000022$, allowing us to reject the $H_0$ that all coefficients in the model are 0.

## Regression Assumption Checks

### 
  In this section, we assess whether our multiple regression model meets key assumptions and take the necessary steps to address any violations of these assumptions. Early visualizations of the distribution of the predictors PCTBACHMOR, NBELOWPOV100, PCTVACANT, and PCTSINGLES and the dependent variable MEDHVAL were presented by histograms which all showed positively-skewed distributions for all predictors. While multiple regression assumptions require the normality of residuals and not predictor values, non-normal distribution of predictors values can indicate violations of the assumptions of non-normal residuals and a lack of linearity.

### 
```{r ii. predictor scatter plots, fig.width=8, fig.height=6}
#| echo: false

     p5 <- ggplot(data = regression_data, aes(x =PCTBACHMOR, y = MEDHVAL)) +
     geom_point(shape=1)+
        labs(
        title = "Median House Value by\n Percentage with Bachelors",
        x= "Percentage with Bachelors",
        y= "Median House Value"
        )+
       theme_minimal()+
       theme(
       panel.grid = element_blank(),
       plot.title = element_text(face = "bold"))
     
     p6 <- ggplot(data = regression_data, aes(x =NBELPOV100, y = MEDHVAL)) +
      geom_point(shape=1)+
        labs(
        title = "Median House Value by\n Number Below Poverty",
        x= "Number Below Poverty",
        y= "Median House Value"
        )+
       theme_minimal()+
       theme(
       panel.grid = element_blank(),
       plot.title = element_text(face = "bold"))
     
     p7 <- ggplot(data = regression_data, aes(x =PCTVACANT, y = MEDHVAL)) +
      geom_point(shape=1)+
        labs(
        title = "Median House Value by\n Percentage of Vacant Units",
        x= "Percentage Vacant",
        y= "Median House Value"
        )+
       theme_minimal()+
       theme(
        panel.grid = element_blank(),  
       plot.title = element_text(face = "bold"))

     p8 <- ggplot(data = regression_data, aes(x =PCTSINGLES, y = MEDHVAL)) +
      geom_point(shape=1)+
        labs(
        title = "Median House Value by\n Percentage of Single Units",
        x= "Percentage of Singles",
        y= "Median House Value"
        )+
       theme_minimal()+
       theme(
       panel.grid = element_blank(),
       plot.title = element_text(face = "bold"))
 
(p5 |p6 ) / (p7| p8)
```

  The skewedness of histograms of each predictor is reflected in the scatter plots of the predictors by the dependent variable median house value, MEDHVAL, as, as the predictor values increase, y values cluster towards lower values. The lack of linearity between the predictor and MEDHVAL and the skewed distribution of predictor values suggest that some type of nonlinear transformation may need to occur in order to normally distribute values and achieve linearity. We performed log transformations which are commonly used to correct the positively skewed distributions evident in our variables.

### 
```{r iii. std residuals histogram}
#| echo: false
hist(Regression_shpData$stdres,
     main = "Histogram of Standardized Regression Residuals",
     xlab = "Standardized Residuals",
     ylab = "Frequency")
```
  We applied logarithmic transformations to all predictors and the dependent variable to see whether the transformations would improve distribution of their values and subsequently allow us to assume linearity and residual normality. We only substituted the log-transformed MEDHVAL (LNMEDHVAL) and log-transformed NBELOPOV (LNNBELOPOV) for the rest of our analysis. The log-transformed predictors PCTVACANT, PCTSINGLES, and PCTBACHMOR did not return an improvement and instead produced zero-inflated distributions. We proceeded to calculate our standardized residuals with the new model of original predictors PCTVACANT, PCTSINGLES, and PCTBACHMOR and with our log-transformed predictor LNNBELPOV by our log transformed dependent variable LNMEDHVAL. The histogram of the standardized residuals show the normality in residuals needed per our assumption and support the need for the logarithmic transformations.

###  
```{r iv. std residual scatter plot}
#| echo: false
plot(Regression_shpData$predvals, Regression_shpData$stdres,
     main = "Standardized Residuals By Predicted Values",
     xlab = "Predicted Values",
     ylab = "Standardized Residuals")
```
  Standardized residuals are residuals divided by their standard deviation as a means to prime residuals across different observations for comparison through normalization. The scatter plot of our standardized residuals shows general homoscedasticity or consistent variance of residuals. There is general uniformity of the standardized residuals as most lie between -2 and positive 2. There are some outliers that extend past -4 and 4 but they do not dominate the overall pattern. There is also no funneling affect or any other pattern of non-constant variance. Thus, our model satisfies the assumption of homoscedasticity of residuals.

###
  Initial spatial visualizations of the dependent and predictor variables suggest that there may be some spatial autocorrelation between their respective measurements. The choropleth map of the logged dependent variable LNMEDHVAL shows that lower values seem to be concentrated in parts of North, Southwest, and West Philadelphia while higher values were clustered in Upper North Philadelphia. The choropleth map of the predictor PCTSINGLES shows higher percentages in parts of Upper North and Northeast Philadelphia. The choropleth map of the predictor PCTBACHMOR shows higher percentages in parts of Upper North and Center Philadelphia. The choropleth map of the logged predictor LNNBELPOV showed lower values in parts of  Upper North, Northeast, and Center Philadelphia. The choropleth map of the predictor PCTVACANT shows higher percentages in parts of North, West, Southwest, South, and Center Philadelphia.This visual inspection suggests that block groups might not be entirely independent of each other and could require further spatial assessment.

### 
```{r vi. std residual choropleth, fig.width=8, fig.height=6}
#| echo: false
ggplot(Regression_shpData) +
  geom_sf(aes(fill =  stdres), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Standardized\nResiduals")+
  labs(
    title = "Map of Standardized Regression Residuals"
  ) +
  theme_void()+
  theme(
    plot.title = element_text(face = "bold", size=16, hjust = 0.7))


```
  The choropleth of standardized residuals suggests possible spatial autocorrelation as there seems to be a concentration of lower values in the southern half of Philadelphia. Visually, there seems to be a gradient effect stemming outward from North Philadelphia into West, Southwest, and South Philadelphia. This indicates that their could be additional factors producing systematic under prediction in the southern half of Philadelphia, especially in North Philadelphia. 

## Additional Models

###
**Stepwise Regression ANOVA table**
```{r i. stepwise results}
#| echo: false
step <- stepAIC(reg1, direction="both")

step$anova

```


  Our initial model before performing stepwise regression:
$$
\text{LNMEDHVAL} \sim \text{PCTVACANT} + \text{PCTSINGLES} + \text{PCTBACHMOR} + \text{LNNBELPOV}
$$
As mentioned earlier, stepwise regression based on AIC evaluates whether a predictor improves the model fit by reducing the AIC. Our initial model had an AIC of -3448.162. When PCTSINGLES was removed, the AIC increased to ‚Äì3432.3. When LNNBELPOV was removed, the AIC increased to ‚Äì3365.0. When PCTVACANT was removed, the AIC increased to  ‚Äì3102.8. When our last predictor PCTBACHMOR was removed, the AIC increased drastically to ‚Äì2379.0. Since the removal of each predictor resulted in a higher AIC, all four initial predictors were retained in the final model. This suggests that the initial model was selected by stepwise regression as being a model that balances explanatory power and complexity.

###
**K-fold Cross-validation Table**
```{r ii. cross-validation}
#| echo: false
train_control <- trainControl(method = "cv", number = 5)

cv_model <- train(LNMEDHVAL ~ PCTVACANT + PCTSINGLES + PCTBACHMOR + LNNBELPOV,
                  data = Regression_shpData,
                  method = "lm",
                  trControl = train_control)
cv_model

cv_model_2 <- train(LNMEDHVAL ~ PCTVACANT + MEDHHINC,
                  data = Regression_shpData,
                  method = "lm",
                  trControl = train_control)
cv_model_2
```


  We performed 5 fold cross-validation on two models, the first model including all of our original predictors and the second model being a reduced set of predictors that alternatively included MEDHHINC as a predictor. The second model is as follows:

$$
\text{LNMEDHVAL} \sim \text{PCTVACANT} + \text{MEDHHINC}
$$ 

The original model yielded a RMSE of 0.368 while the reduced model yielded a RMSE of 0.443, signaling that the additional predictors in the full model had better predictive power compared to  PCTVACANT and MEDHHINC alone. 

# Discussion & Limitations

